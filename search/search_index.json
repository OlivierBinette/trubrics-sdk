{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the trubrics-sdk trubrics.com Minimise AI Risk, Maximise Adoption The trubrics-sdk is a python library for validating machine learning with data science and domain expertise. This is achieved by collecting business user feedback, creating ML validations with this feedback, and building repeatable validation checklists - a trubric. Key Features ML model validation with the ModelValidator Feedback collection on ML models / data from users with the FeedbackCollector Tracking and management of validation runs and feedback in the Trubrics platform Install (Python 3.7+) (venv) $ pip install trubrics Validate a model with the ModelValidator There are three basic steps to creating model validations with the trubrics-sdk: Initialise a DataContext , that wraps ML datasets and metadata into a trubrics friendly object. Build validations with the ModelValidator , using the DataContext and any ML model (scikit-learn or any python model ). The ModelValidator holds a number of out-of-the-box validations and can also be used to build custom validations from a python function. Group validations into a Trubric , which is saved as a .json file and rerun against any model / dataset. Try out these steps by creating your own Trubric with this example: from trubrics.context import DataContext from trubrics.example import get_titanic_data_and_model from trubrics.validations import ModelValidator , Trubric _ , test_df , model = get_titanic_data_and_model () # 1. Init DataContext data_context = DataContext ( testing_data = test_df , # pandas dataframe of data to validate model on target = \"Survived\" , ) # 2. Build validations with ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) validations = [ model_validator . validate_performance_against_threshold ( metric = \"accuracy\" , threshold = 0.7 ), model_validator . validate_feature_in_top_n_important_features ( feature = \"Age\" , top_n_features = 3 ), ] # 3. Group validations into a Trubric trubric = Trubric ( name = \"my_first_trubric\" , data_context_name = data_context . name , data_context_version = data_context . version , validations = validations , ) trubric . save_local ( path = \"./my_first_trubric.json\" ) # save trubric as a local .json file trubric . save_ui () # or to the Trubrics platform The Trubric defines the gold standard of validations required for your project, and may be used to validate any combination of model and DataContext . Once saved as a .json, the trubric may be run directly from the CLI . See a full tutorial on the titanic dataset here . Collect user feedback with the FeedbackCollector Trubrics feedback components help you collect feedback on your models with your favourite python web development library. Once feedback has been collected from business users, it should be translated into validation points to ensure repeatable validations throughout the lifetime of the model. Add the trubrics feedback component to your ML apps now to start collecting feedback: Framework Getting Started Code Snippets [Streamlit](https://streamlit.io/) from trubrics.feedback import collect_feedback_streamlit collect_feedback_streamlit ( path = \"./feedback_issue.json\" , # path to save feedback .json tags = [ \"streamlit\" ], metadata = { \"some\" : \"metadata\" }, save_ui = False , # set to True to save feedback to Trubrics ) Dash and Gradio integrations Framework Getting Started Code Snippets [Dash](https://dash.plotly.com/) from dash import Dash , html from trubrics.feedback import collect_feedback_dash app = Dash ( __name__ ) app . layout = html . Div ( [ collect_feedback_dash () ] ) if __name__ == \"__main__\" : app . run_server ( debug = True ) [Gradio](https://gradio.app/) import gradio as gr from trubrics.feedback import collect_feedback_gradio with gr . Blocks () as demo : collect_feedback_gradio () demo . launch () You can view our demo user feedback app, using the streamlit feedback collector and an example experimentation tool, on the titanic dataset & model on Hugging Face Spaces , or run it locally with the CLI command: (venv) $ trubrics example-app Track all validation runs and feedback in Trubrics The Trubrics platform allows teams to collaborate on model issues and track validation changes. Please get in touch with us here to gain access to Trubrics for you and your team. trubrics init will initialise your terminal and authenticate with your Trubrics account trubrics run will run your validations from the terminal and track them in Trubrics Watch our getting started demo","title":"Getting Started"},{"location":"#welcome-to-the-trubrics-sdk","text":"trubrics.com Minimise AI Risk, Maximise Adoption The trubrics-sdk is a python library for validating machine learning with data science and domain expertise. This is achieved by collecting business user feedback, creating ML validations with this feedback, and building repeatable validation checklists - a trubric.","title":"Welcome to the trubrics-sdk"},{"location":"#key-features","text":"ML model validation with the ModelValidator Feedback collection on ML models / data from users with the FeedbackCollector Tracking and management of validation runs and feedback in the Trubrics platform","title":"Key Features"},{"location":"#install-python-37","text":"(venv) $ pip install trubrics","title":"Install (Python 3.7+)"},{"location":"#validate-a-model-with-the-modelvalidator","text":"There are three basic steps to creating model validations with the trubrics-sdk: Initialise a DataContext , that wraps ML datasets and metadata into a trubrics friendly object. Build validations with the ModelValidator , using the DataContext and any ML model (scikit-learn or any python model ). The ModelValidator holds a number of out-of-the-box validations and can also be used to build custom validations from a python function. Group validations into a Trubric , which is saved as a .json file and rerun against any model / dataset. Try out these steps by creating your own Trubric with this example: from trubrics.context import DataContext from trubrics.example import get_titanic_data_and_model from trubrics.validations import ModelValidator , Trubric _ , test_df , model = get_titanic_data_and_model () # 1. Init DataContext data_context = DataContext ( testing_data = test_df , # pandas dataframe of data to validate model on target = \"Survived\" , ) # 2. Build validations with ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) validations = [ model_validator . validate_performance_against_threshold ( metric = \"accuracy\" , threshold = 0.7 ), model_validator . validate_feature_in_top_n_important_features ( feature = \"Age\" , top_n_features = 3 ), ] # 3. Group validations into a Trubric trubric = Trubric ( name = \"my_first_trubric\" , data_context_name = data_context . name , data_context_version = data_context . version , validations = validations , ) trubric . save_local ( path = \"./my_first_trubric.json\" ) # save trubric as a local .json file trubric . save_ui () # or to the Trubrics platform The Trubric defines the gold standard of validations required for your project, and may be used to validate any combination of model and DataContext . Once saved as a .json, the trubric may be run directly from the CLI . See a full tutorial on the titanic dataset here .","title":"Validate a model with the ModelValidator"},{"location":"#collect-user-feedback-with-the-feedbackcollector","text":"Trubrics feedback components help you collect feedback on your models with your favourite python web development library. Once feedback has been collected from business users, it should be translated into validation points to ensure repeatable validations throughout the lifetime of the model. Add the trubrics feedback component to your ML apps now to start collecting feedback: Framework Getting Started Code Snippets [Streamlit](https://streamlit.io/) from trubrics.feedback import collect_feedback_streamlit collect_feedback_streamlit ( path = \"./feedback_issue.json\" , # path to save feedback .json tags = [ \"streamlit\" ], metadata = { \"some\" : \"metadata\" }, save_ui = False , # set to True to save feedback to Trubrics ) Dash and Gradio integrations Framework Getting Started Code Snippets [Dash](https://dash.plotly.com/) from dash import Dash , html from trubrics.feedback import collect_feedback_dash app = Dash ( __name__ ) app . layout = html . Div ( [ collect_feedback_dash () ] ) if __name__ == \"__main__\" : app . run_server ( debug = True ) [Gradio](https://gradio.app/) import gradio as gr from trubrics.feedback import collect_feedback_gradio with gr . Blocks () as demo : collect_feedback_gradio () demo . launch () You can view our demo user feedback app, using the streamlit feedback collector and an example experimentation tool, on the titanic dataset & model on Hugging Face Spaces , or run it locally with the CLI command: (venv) $ trubrics example-app","title":"Collect user feedback with the FeedbackCollector"},{"location":"#track-all-validation-runs-and-feedback-in-trubrics","text":"The Trubrics platform allows teams to collaborate on model issues and track validation changes. Please get in touch with us here to gain access to Trubrics for you and your team.","title":"Track all validation runs and feedback in Trubrics"},{"location":"#trubrics-init-will-initialise-your-terminal-and-authenticate-with-your-trubrics-account","text":"","title":"trubrics init will initialise your terminal and authenticate with your Trubrics account"},{"location":"#trubrics-run-will-run-your-validations-from-the-terminal-and-track-them-in-trubrics","text":"","title":"trubrics run will run your validations from the terminal and track them in Trubrics"},{"location":"#watch-our-getting-started-demo","text":"","title":"Watch our getting started demo"},{"location":"custom_validations/","text":"Building custom validations All ML projects are unique, and custom validations should strongly be considered from the start. They allow you to tailor trubrics to your needs, and transform user feedback into meaningful validations. The ModelValidator object can be used to build your own custom validations, by creating a class that inherits from ModelValidator as per the example: Example from trubrics.context import DataContext from trubrics.validations import ModelValidator from trubrics.validations.validation_output import ( validation_output , validation_output_type , ) class CustomValidator ( ModelValidator ): def __init__ ( self , data : DataContext , model , custom_scorers = None , slicing_functions = None ): super () . __init__ ( data , model , custom_scorers , slicing_functions ) def _validate_master_age ( self , age_limit_master ) -> validation_output_type : \"\"\" Write your custom validation function here. Notes ----- This method is separated from validate_performance_for_different_fares to apply @validation_output and for unit testing. The @validation_output decorator allows you to generate a Validation object, and must be used to be able to save your validation as part of a Trubric. This decorator requires you to return values with the same type as validation_output_type. \"\"\" master_df = self . tm . data . testing_data . loc [ lambda df : df [ \"Title\" ] == \"Master\" ] errors_df = master_df . loc [ lambda df : df [ \"Age\" ] >= age_limit_master ] return len ( errors_df ) == 0 , { \"errors_df\" : errors_df . to_dict ()} @validation_output def validate_master_age ( self , age_limit_master : int , severity = None ): \"\"\"Validate that passengers with the title \"master\" are younger than a certain age Args: age_limit_master: cut off value for master Returns: True for success, false otherwise. With a results dictionary giving dict of errors. \"\"\" return self . _validate_master_age ( age_limit_master ) Software conventions of the ModelValidator For best practices on creating a CustomValidator, follow the same conventions as the ModelValidator : Contains all code for validations All validation names start with the prefix \"validate_\" Validations are built with two methods: A method with an underscored prefix e.g. \"_validate_something\". This method contains all code logic for the validation, and must return a boolean variable that indicates a pass / fail of the validation, and a dictionary that holds contextual information that is calculated during the validation run. This method is unit tested and documented. The second method has the same name, without the underscore prefix e.g. \"validate_something\". This method is used to call the validation, and formats the output with the validation_output decorator . The @validation_output decorator Example of a validation method decorated with @validation_output @validation_output def validate_something ( self , some_arg , some_kwarg ): return self . _validate_something ( some_arg , some_kwarg ) The @validation_output decorator transforms the output of a validation function into a Validation *. For this transformation, it is necessary for the validation function to respect the return type (outcome, result) -> Tuple[bool, Dict[str, Union[str, int, float]]] , where the outcome represents whether the validation has passed or fail, and the result is any metadata that has been computed during the validation that can be useful for giving greater context as to why a validation has passed or failed. *Validation Bases: BaseModel Dataclass for a single validation point. Must be serialisable to .json, as is fed into Trubric dataclass. Note A Validation object constrains the output of validations, with the @validation_output decorator. Attributes: Name Type Description validation_type str method name of the validation. validation_kwargs Dict [ str , Optional [ Any ]] all args and kwargs that the validation had run with. explanation str docstring explanation of the validation. outcome str pass or fail output of the validation. severity str severity of the validation, can be one of [\"error\", \"warning\", \"experiment\"], is \"error\" by default result Optional [ Dict [ str , Optional [ Any ]]] a dictionary of contextual elements calculated during the validation run Source code in trubrics/validations/dataclass.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class Validation ( BaseModel ): \"\"\" Dataclass for a single validation point. Must be serialisable to .json, as is fed into Trubric dataclass. Note: A Validation object constrains the output of validations, with the @validation_output decorator. Attributes: validation_type: method name of the validation. validation_kwargs: all args and kwargs that the validation had run with. explanation: docstring explanation of the validation. outcome: pass or fail output of the validation. severity: severity of the validation, can be one of [\"error\", \"warning\", \"experiment\"], is \"error\" by default result: a dictionary of contextual elements calculated during the validation run \"\"\" validation_type : str validation_kwargs : Dict [ str , Optional [ Any ]] explanation : str outcome : str severity : str = \"error\" result : Optional [ Dict [ str , Optional [ Any ]]] class Config : extra = \"forbid\" validate_assignment = True schema_extra = _validation_context_example () @validator ( \"severity\" ) def severity_must_be ( cls , v : str ): severity_values = [ \"error\" , \"warning\" , \"experiment\" ] if v not in severity_values : raise KeyError ( f \"Severity must be set to: { severity_values } .\" ) return v @validator ( \"outcome\" ) def outcome_must_be ( cls , v : str ): outcome_values = [ \"pass\" , \"fail\" ] if v not in outcome_values : raise KeyError ( f \"Outcome must be set to: { outcome_values } .\" ) return v","title":"Custom validations"},{"location":"custom_validations/#building-custom-validations","text":"All ML projects are unique, and custom validations should strongly be considered from the start. They allow you to tailor trubrics to your needs, and transform user feedback into meaningful validations. The ModelValidator object can be used to build your own custom validations, by creating a class that inherits from ModelValidator as per the example: Example from trubrics.context import DataContext from trubrics.validations import ModelValidator from trubrics.validations.validation_output import ( validation_output , validation_output_type , ) class CustomValidator ( ModelValidator ): def __init__ ( self , data : DataContext , model , custom_scorers = None , slicing_functions = None ): super () . __init__ ( data , model , custom_scorers , slicing_functions ) def _validate_master_age ( self , age_limit_master ) -> validation_output_type : \"\"\" Write your custom validation function here. Notes ----- This method is separated from validate_performance_for_different_fares to apply @validation_output and for unit testing. The @validation_output decorator allows you to generate a Validation object, and must be used to be able to save your validation as part of a Trubric. This decorator requires you to return values with the same type as validation_output_type. \"\"\" master_df = self . tm . data . testing_data . loc [ lambda df : df [ \"Title\" ] == \"Master\" ] errors_df = master_df . loc [ lambda df : df [ \"Age\" ] >= age_limit_master ] return len ( errors_df ) == 0 , { \"errors_df\" : errors_df . to_dict ()} @validation_output def validate_master_age ( self , age_limit_master : int , severity = None ): \"\"\"Validate that passengers with the title \"master\" are younger than a certain age Args: age_limit_master: cut off value for master Returns: True for success, false otherwise. With a results dictionary giving dict of errors. \"\"\" return self . _validate_master_age ( age_limit_master )","title":"Building custom validations"},{"location":"custom_validations/#software-conventions-of-the-modelvalidator","text":"For best practices on creating a CustomValidator, follow the same conventions as the ModelValidator : Contains all code for validations All validation names start with the prefix \"validate_\" Validations are built with two methods: A method with an underscored prefix e.g. \"_validate_something\". This method contains all code logic for the validation, and must return a boolean variable that indicates a pass / fail of the validation, and a dictionary that holds contextual information that is calculated during the validation run. This method is unit tested and documented. The second method has the same name, without the underscore prefix e.g. \"validate_something\". This method is used to call the validation, and formats the output with the validation_output decorator .","title":"Software conventions of the ModelValidator"},{"location":"custom_validations/#the-validation_output-decorator","text":"Example of a validation method decorated with @validation_output @validation_output def validate_something ( self , some_arg , some_kwarg ): return self . _validate_something ( some_arg , some_kwarg ) The @validation_output decorator transforms the output of a validation function into a Validation *. For this transformation, it is necessary for the validation function to respect the return type (outcome, result) -> Tuple[bool, Dict[str, Union[str, int, float]]] , where the outcome represents whether the validation has passed or fail, and the result is any metadata that has been computed during the validation that can be useful for giving greater context as to why a validation has passed or failed. *Validation Bases: BaseModel Dataclass for a single validation point. Must be serialisable to .json, as is fed into Trubric dataclass. Note A Validation object constrains the output of validations, with the @validation_output decorator. Attributes: Name Type Description validation_type str method name of the validation. validation_kwargs Dict [ str , Optional [ Any ]] all args and kwargs that the validation had run with. explanation str docstring explanation of the validation. outcome str pass or fail output of the validation. severity str severity of the validation, can be one of [\"error\", \"warning\", \"experiment\"], is \"error\" by default result Optional [ Dict [ str , Optional [ Any ]]] a dictionary of contextual elements calculated during the validation run Source code in trubrics/validations/dataclass.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class Validation ( BaseModel ): \"\"\" Dataclass for a single validation point. Must be serialisable to .json, as is fed into Trubric dataclass. Note: A Validation object constrains the output of validations, with the @validation_output decorator. Attributes: validation_type: method name of the validation. validation_kwargs: all args and kwargs that the validation had run with. explanation: docstring explanation of the validation. outcome: pass or fail output of the validation. severity: severity of the validation, can be one of [\"error\", \"warning\", \"experiment\"], is \"error\" by default result: a dictionary of contextual elements calculated during the validation run \"\"\" validation_type : str validation_kwargs : Dict [ str , Optional [ Any ]] explanation : str outcome : str severity : str = \"error\" result : Optional [ Dict [ str , Optional [ Any ]]] class Config : extra = \"forbid\" validate_assignment = True schema_extra = _validation_context_example () @validator ( \"severity\" ) def severity_must_be ( cls , v : str ): severity_values = [ \"error\" , \"warning\" , \"experiment\" ] if v not in severity_values : raise KeyError ( f \"Severity must be set to: { severity_values } .\" ) return v @validator ( \"outcome\" ) def outcome_must_be ( cls , v : str ): outcome_values = [ \"pass\" , \"fail\" ] if v not in outcome_values : raise KeyError ( f \"Outcome must be set to: { outcome_values } .\" ) return v","title":"The @validation_output decorator"},{"location":"data_context/","text":"The DataContext Data sits at the heart of all ML projects, thus also playing a central role in using the trubrics-sdk. We have decided to group all ML datasets and metadata into a single object: the DataContext . Initialising this object is the first step to both building validations with ModelValidator and collecting user feedback with the FeedbackCollector . Getting started with the DataContext The most basic level of ML validation can be performed on a single dataset (of data that has not been seen during model training) and a single model . This testing_data dataset must contain: all features that were used to train the model the target variable to predict This is the minimum requirement for the DataContext to start building validations: Example from trubrics.context import DataContext data_context = DataContext ( testing_data = test_df , # pandas dataframe of data to validate model on target = \"target_variable\" , # name of target variable within the testing_data dataset ) There are many other attributes to the DataContext that can be used in validations / collecting feedback: DataContext object Bases: BaseModel The DataContext wraps ML datasets into a trubrics friendly format. Note The DataContext must contain at least a testing_data and a target attribute. Default values are set for all other attributes. Attributes: Name Type Description name str DataContext name. Required for trubrics UI tracking. version str DataContext version. Required for trubrics UI tracking. testing_data pd . DataFrame Dataframe that all validations are executed against. Should contain all features and target values that the model was trained with. target str Name of target column. training_data Optional [ pd . DataFrame ] Dataframe of the training data. minimum_functionality_data Optional [ pd . DataFrame ] Dataframe of the minimum functionality of a model. This contains samples that the model should never fail on. features Optional [ List [ str ]] list of model features. If None, all columns except the target column are set as features. categorical_columns Optional [ List [ str ]] List of categorical names of the train & test datasets. business_columns Optional [ Dict [ str , str ]] Mapping between dataset column names and comprehensible column names to be displayed to users. Source code in trubrics/context.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class DataContext ( BaseModel ): \"\"\" The DataContext wraps ML datasets into a trubrics friendly format. Note: The DataContext *must contain* at least a testing_data and a target attribute. Default values are set for all other attributes. Attributes: name: DataContext name. Required for trubrics UI tracking. version: DataContext version. Required for trubrics UI tracking. testing_data: Dataframe that all validations are executed against. Should contain all features and target values that the model was trained with. target: Name of target column. training_data: Dataframe of the training data. minimum_functionality_data: Dataframe of the minimum functionality of a model. This contains samples that the model should never fail on. features: list of model features. If None, all columns except the target column are set as features. categorical_columns: List of categorical names of the train & test datasets. business_columns: Mapping between dataset column names and comprehensible column names to be displayed to users. \"\"\" name : str = \"my_dataset\" version : str = \"v0.0.1\" testing_data : pd . DataFrame target : str training_data : Optional [ pd . DataFrame ] = None minimum_functionality_data : Optional [ pd . DataFrame ] = None features : Optional [ List [ str ]] = None categorical_columns : Optional [ List [ str ]] = None business_columns : Optional [ Dict [ str , str ]] = None class Config : allow_mutation = False arbitrary_types_allowed = True extra = \"forbid\" @property def X_test ( self ) -> pd . DataFrame : \"\"\"Feature testing dataframe.\"\"\" return self . testing_data [ self . features ] @property def y_test ( self ) -> pd . Series : \"\"\"Target testing series.\"\"\" return self . testing_data [ self . target ] @property def X_train ( self ) -> Optional [ pd . DataFrame ]: \"\"\"Feature training dataframe.\"\"\" return self . training_data [ self . features ] if self . training_data is not None else None @property def y_train ( self ) -> Optional [ pd . Series ]: \"\"\"Target training series.\"\"\" return self . training_data [ self . target ] if self . training_data is not None else None @property def renamed_testing_data ( self ) -> pd . DataFrame : \"\"\"Renamed testing data with business columns\"\"\" if self . business_columns is None : raise TypeError ( \"Business columns must be set to rename testing features.\" ) return self . testing_data . rename ( columns = self . business_columns ) @root_validator def validate_features ( cls , values ) -> List [ str ]: \"\"\"Features are here defined as all testing column names excluding the target feature. Note: @root_validator is used here to allow for @properties to use self.features. \"\"\" v = values [ \"features\" ] if v is None : values [ \"features\" ] = [ col for col in values [ \"testing_data\" ] . columns if col != values [ \"target\" ]] elif set ( v ) . issubset ( set ( values [ \"testing_data\" ] . columns )): values [ \"features\" ] = v else : raise PandasSchemaError ( \"All features must be present in testing_data.\" ) return values @validator ( \"target\" ) def target_column_must_be_in_data ( cls , v , values ): if v not in values [ \"testing_data\" ] . columns : raise KeyError ( \"Target column must be in testing_data column names.\" ) return v @validator ( \"training_data\" ) def training_and_testing_must_have_same_schema ( cls , v , values ): if v is not None and not schema_is_equal ( values [ \"testing_data\" ], v ): raise PandasSchemaError ( \"Training and testing data must have identical schemas.\" ) return v @validator ( \"minimum_functionality_data\" ) def minimum_functionality_and_testing_must_have_same_schema ( cls , v , values ): if v is not None and not schema_is_equal ( values [ \"testing_data\" ], v ): raise PandasSchemaError ( \"Minimum functionality and testing data must have identical schemas.\" ) return v @validator ( \"categorical_columns\" ) def categorical_columns_must_be_in_data ( cls , v , values ): if not set ( v ) . issubset ( values [ \"testing_data\" ] . columns ): raise KeyError ( \"All categorical columns must be in testing_data column names.\" ) return v @validator ( \"categorical_columns\" ) def target_column_must_not_be_in_categoricals ( cls , v , values ): if v is not None and values [ \"target\" ] in v : raise ValueError ( \"Target column should not feature as a categorical column. Categorical columns only refer to features.\" ) return v @validator ( \"business_columns\" ) def business_columns_must_be_in_data ( cls , v , values ): if not set ( v . keys ()) . issubset ( values [ \"testing_data\" ] . columns ): raise KeyError ( \"All business columns must be in testing_data column names.\" ) return v DataContext versioning Versioning the different datasets in a DataContext is currently left to the developer. There are name and version attributes that will allow the tracking of the DataContext throughout, eventually being fed into a saved Trubric .json file. Example from trubrics.context import DataContext data_context = DataContext ( name = \"titanic\" , version = \"0.3\" , testing_data = test_df , target = \"target_variable\" , ) Training & Minimum Functionality datasets There are two additional datasets that can be used in the DataContext for different validations : training_data : can be used interchangeably with testing_data in most performance or feature importance validations. minimum_functionality_data : for minimum functionality validations. Example from trubrics.context import DataContext data_context = DataContext ( testing_data = test_df , target = \"target_variable\" , training_data = train_df , minimum_functionality_data = minimum_functionality_df , ) Data features metadata There are two extra attributes that hold metadata: features : used for specifying the features of the model. This can be useful for when making data slices on a specific column that is not a model feature, for example. categorical_columns : used for distinguishing between different input widgets in the what-if component business_columns : used for renaming dataset columns with a different name for business user understanding Example from trubrics.context import DataContext data_context = DataContext ( testing_data = test_df , target = \"target_variable\" , features = [ \"feature_a\" , \"feature_b\" ], categorical_columns = [ \"feature_a\" , \"feature_b\" ], business_columns = { \"feature_a\" : \"renamed_feature_a\" }, )","title":"The DataContext"},{"location":"data_context/#the-datacontext","text":"Data sits at the heart of all ML projects, thus also playing a central role in using the trubrics-sdk. We have decided to group all ML datasets and metadata into a single object: the DataContext . Initialising this object is the first step to both building validations with ModelValidator and collecting user feedback with the FeedbackCollector .","title":"The DataContext"},{"location":"data_context/#getting-started-with-the-datacontext","text":"The most basic level of ML validation can be performed on a single dataset (of data that has not been seen during model training) and a single model . This testing_data dataset must contain: all features that were used to train the model the target variable to predict This is the minimum requirement for the DataContext to start building validations: Example from trubrics.context import DataContext data_context = DataContext ( testing_data = test_df , # pandas dataframe of data to validate model on target = \"target_variable\" , # name of target variable within the testing_data dataset ) There are many other attributes to the DataContext that can be used in validations / collecting feedback: DataContext object Bases: BaseModel The DataContext wraps ML datasets into a trubrics friendly format. Note The DataContext must contain at least a testing_data and a target attribute. Default values are set for all other attributes. Attributes: Name Type Description name str DataContext name. Required for trubrics UI tracking. version str DataContext version. Required for trubrics UI tracking. testing_data pd . DataFrame Dataframe that all validations are executed against. Should contain all features and target values that the model was trained with. target str Name of target column. training_data Optional [ pd . DataFrame ] Dataframe of the training data. minimum_functionality_data Optional [ pd . DataFrame ] Dataframe of the minimum functionality of a model. This contains samples that the model should never fail on. features Optional [ List [ str ]] list of model features. If None, all columns except the target column are set as features. categorical_columns Optional [ List [ str ]] List of categorical names of the train & test datasets. business_columns Optional [ Dict [ str , str ]] Mapping between dataset column names and comprehensible column names to be displayed to users. Source code in trubrics/context.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class DataContext ( BaseModel ): \"\"\" The DataContext wraps ML datasets into a trubrics friendly format. Note: The DataContext *must contain* at least a testing_data and a target attribute. Default values are set for all other attributes. Attributes: name: DataContext name. Required for trubrics UI tracking. version: DataContext version. Required for trubrics UI tracking. testing_data: Dataframe that all validations are executed against. Should contain all features and target values that the model was trained with. target: Name of target column. training_data: Dataframe of the training data. minimum_functionality_data: Dataframe of the minimum functionality of a model. This contains samples that the model should never fail on. features: list of model features. If None, all columns except the target column are set as features. categorical_columns: List of categorical names of the train & test datasets. business_columns: Mapping between dataset column names and comprehensible column names to be displayed to users. \"\"\" name : str = \"my_dataset\" version : str = \"v0.0.1\" testing_data : pd . DataFrame target : str training_data : Optional [ pd . DataFrame ] = None minimum_functionality_data : Optional [ pd . DataFrame ] = None features : Optional [ List [ str ]] = None categorical_columns : Optional [ List [ str ]] = None business_columns : Optional [ Dict [ str , str ]] = None class Config : allow_mutation = False arbitrary_types_allowed = True extra = \"forbid\" @property def X_test ( self ) -> pd . DataFrame : \"\"\"Feature testing dataframe.\"\"\" return self . testing_data [ self . features ] @property def y_test ( self ) -> pd . Series : \"\"\"Target testing series.\"\"\" return self . testing_data [ self . target ] @property def X_train ( self ) -> Optional [ pd . DataFrame ]: \"\"\"Feature training dataframe.\"\"\" return self . training_data [ self . features ] if self . training_data is not None else None @property def y_train ( self ) -> Optional [ pd . Series ]: \"\"\"Target training series.\"\"\" return self . training_data [ self . target ] if self . training_data is not None else None @property def renamed_testing_data ( self ) -> pd . DataFrame : \"\"\"Renamed testing data with business columns\"\"\" if self . business_columns is None : raise TypeError ( \"Business columns must be set to rename testing features.\" ) return self . testing_data . rename ( columns = self . business_columns ) @root_validator def validate_features ( cls , values ) -> List [ str ]: \"\"\"Features are here defined as all testing column names excluding the target feature. Note: @root_validator is used here to allow for @properties to use self.features. \"\"\" v = values [ \"features\" ] if v is None : values [ \"features\" ] = [ col for col in values [ \"testing_data\" ] . columns if col != values [ \"target\" ]] elif set ( v ) . issubset ( set ( values [ \"testing_data\" ] . columns )): values [ \"features\" ] = v else : raise PandasSchemaError ( \"All features must be present in testing_data.\" ) return values @validator ( \"target\" ) def target_column_must_be_in_data ( cls , v , values ): if v not in values [ \"testing_data\" ] . columns : raise KeyError ( \"Target column must be in testing_data column names.\" ) return v @validator ( \"training_data\" ) def training_and_testing_must_have_same_schema ( cls , v , values ): if v is not None and not schema_is_equal ( values [ \"testing_data\" ], v ): raise PandasSchemaError ( \"Training and testing data must have identical schemas.\" ) return v @validator ( \"minimum_functionality_data\" ) def minimum_functionality_and_testing_must_have_same_schema ( cls , v , values ): if v is not None and not schema_is_equal ( values [ \"testing_data\" ], v ): raise PandasSchemaError ( \"Minimum functionality and testing data must have identical schemas.\" ) return v @validator ( \"categorical_columns\" ) def categorical_columns_must_be_in_data ( cls , v , values ): if not set ( v ) . issubset ( values [ \"testing_data\" ] . columns ): raise KeyError ( \"All categorical columns must be in testing_data column names.\" ) return v @validator ( \"categorical_columns\" ) def target_column_must_not_be_in_categoricals ( cls , v , values ): if v is not None and values [ \"target\" ] in v : raise ValueError ( \"Target column should not feature as a categorical column. Categorical columns only refer to features.\" ) return v @validator ( \"business_columns\" ) def business_columns_must_be_in_data ( cls , v , values ): if not set ( v . keys ()) . issubset ( values [ \"testing_data\" ] . columns ): raise KeyError ( \"All business columns must be in testing_data column names.\" ) return v","title":"Getting started with the DataContext"},{"location":"data_context/#datacontext-versioning","text":"Versioning the different datasets in a DataContext is currently left to the developer. There are name and version attributes that will allow the tracking of the DataContext throughout, eventually being fed into a saved Trubric .json file. Example from trubrics.context import DataContext data_context = DataContext ( name = \"titanic\" , version = \"0.3\" , testing_data = test_df , target = \"target_variable\" , )","title":"DataContext versioning"},{"location":"data_context/#training-minimum-functionality-datasets","text":"There are two additional datasets that can be used in the DataContext for different validations : training_data : can be used interchangeably with testing_data in most performance or feature importance validations. minimum_functionality_data : for minimum functionality validations. Example from trubrics.context import DataContext data_context = DataContext ( testing_data = test_df , target = \"target_variable\" , training_data = train_df , minimum_functionality_data = minimum_functionality_df , )","title":"Training &amp; Minimum Functionality datasets"},{"location":"data_context/#data-features-metadata","text":"There are two extra attributes that hold metadata: features : used for specifying the features of the model. This can be useful for when making data slices on a specific column that is not a model feature, for example. categorical_columns : used for distinguishing between different input widgets in the what-if component business_columns : used for renaming dataset columns with a different name for business user understanding Example from trubrics.context import DataContext data_context = DataContext ( testing_data = test_df , target = \"target_variable\" , features = [ \"feature_a\" , \"feature_b\" ], categorical_columns = [ \"feature_a\" , \"feature_b\" ], business_columns = { \"feature_a\" : \"renamed_feature_a\" }, )","title":"Data features metadata"},{"location":"feedback/","text":"Gather feedback from business users Trubrics feedback components help you to collect feedback on your models with your favourite python library. Once feedback has been collected from business users, it should be translated into validation points to ensure repeatable checking throughout the lifetime of the model. Trubrics platform access The Trubrics platform will allow you to track all issues, and discuss errors with users and other collaborators. There are also capabilities to close feedback issues by linking to specific validation runs. Creating accounts for users will also allow authentication directly in the FeedbackCollector. Don't hesitate to get in touch with us here to gain access for you and your team. Add a FeedbackCollector to your ML apps now to start collecting feedback: Framework Getting Started Code Snippets [Streamlit](https://streamlit.io/) from trubrics.feedback import collect_feedback_streamlit collect_feedback_streamlit ( path = \"./feedback_issue.json\" , # path to save feedback .json tags = [ \"streamlit\" ], metadata = { \"some\" : \"metadata\" }, save_ui = False , # set to True to save feedback to Trubrics ) Dash and Gradio integrations Framework Getting Started Code Snippets [Dash](https://dash.plotly.com/) from dash import Dash , html from trubrics.feedback import collect_feedback_dash app = Dash ( __name__ ) app . layout = html . Div ( [ collect_feedback_dash () ] ) if __name__ == \"__main__\" : app . run_server ( debug = True ) [Gradio](https://gradio.app/) import gradio as gr from trubrics.feedback import collect_feedback_gradio with gr . Blocks () as demo : collect_feedback_gradio () demo . launch () You can view our demo user feedback app, using the streamlit feedback collector and an example experimentation tool, on the titanic dataset & model on Hugging Face Spaces , or run it locally with the CLI command: (venv) $ trubrics example-app","title":"The FeedbackCollector"},{"location":"feedback/#gather-feedback-from-business-users","text":"Trubrics feedback components help you to collect feedback on your models with your favourite python library. Once feedback has been collected from business users, it should be translated into validation points to ensure repeatable checking throughout the lifetime of the model. Trubrics platform access The Trubrics platform will allow you to track all issues, and discuss errors with users and other collaborators. There are also capabilities to close feedback issues by linking to specific validation runs. Creating accounts for users will also allow authentication directly in the FeedbackCollector. Don't hesitate to get in touch with us here to gain access for you and your team. Add a FeedbackCollector to your ML apps now to start collecting feedback: Framework Getting Started Code Snippets [Streamlit](https://streamlit.io/) from trubrics.feedback import collect_feedback_streamlit collect_feedback_streamlit ( path = \"./feedback_issue.json\" , # path to save feedback .json tags = [ \"streamlit\" ], metadata = { \"some\" : \"metadata\" }, save_ui = False , # set to True to save feedback to Trubrics ) Dash and Gradio integrations Framework Getting Started Code Snippets [Dash](https://dash.plotly.com/) from dash import Dash , html from trubrics.feedback import collect_feedback_dash app = Dash ( __name__ ) app . layout = html . Div ( [ collect_feedback_dash () ] ) if __name__ == \"__main__\" : app . run_server ( debug = True ) [Gradio](https://gradio.app/) import gradio as gr from trubrics.feedback import collect_feedback_gradio with gr . Blocks () as demo : collect_feedback_gradio () demo . launch () You can view our demo user feedback app, using the streamlit feedback collector and an example experimentation tool, on the titanic dataset & model on Hugging Face Spaces , or run it locally with the CLI command: (venv) $ trubrics example-app","title":"Gather feedback from business users"},{"location":"metrics/","text":"Metrics and scoring functions Many validations in the ModelValidator require the computation of a metric to validate. It is good practice to recompute these metrics outside of your training pipeline to avoid errors in passing on pre-computed metrics. As performance computation can be expensive in compute, a given metric is calculated once on a given dataset, and stored in the ModelValidator for use by any following validations. The dataset can be any datasets present in the DataContext , or any slices of these datasets. Example For example, here the ModelValidator computes recall on the test set for the first validation, and then uses this stored value in the second validation. from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) validations = [ model_validator . validate_test_performance_against_dummy ( metric = \"recall\" , strategy = \"stratified\" ) model_validator . validate_performance_between_train_and_test ( metric = \"recall\" , threshold = 0.3 ) ] print ( model_validator . performances ) # all performance values are stored in the performances attribute 1. Scikit-learn scoring functions The ModelValidator makes use of sklearn.metrics to compute model performance on a given dataset. The metric is fed into the validation as a string (as above), to use any of sklearn's scorers. Note List available scorers by running: import sklearn.metrics print ( sklearn . metrics . SCORERS ) 2. Custom scoring functions To create your own scoring functions, scikit-learn provides a make_scorer function. In the case of an error metric, where a lower value is better, make use of the greater_is_better=False argument in order to stay consistent with comparison of metrics in performance validations. Example Here is an example of a custom scoring function being fed into the ModelValidator and used in a performance validation: from trubrics.validations import ModelValidator from sklearn.metrics import accuracy_score , make_scorer def custom_binary_error ( y_true , y_pred ): \"\"\" An example of an error metric for binary classification. \"\"\" accuracy = accuracy_score ( y_true , y_pred ) return round ( 1 - accuracy , 3 ) custom_score = make_scorer ( custom_binary_error , greater_is_better = False ) custom_scorers = { \"my_custom_loss\" : custom_score } model_validator = ModelValidator ( data = data_context , model = model , custom_scorers = custom_scorers ) validations = [ model_validator . validate_test_performance_against_dummy ( metric = \"my_custom_loss\" , strategy = \"stratified\" ) ] 3. Data slicing functions It is often necessary to compute performance on specific splits of a dataset, in order to test for model performance bias for example. Data slicing functions can also be fed into the ModelValidator , and used in validations in the same way that metrics are. Each function must take a single pandas dataframe argument, and return the filtered pandas dataframe. Example Here is an example of a series of data slicing functions being fed into the ModelValidator and used in a validation: from trubrics.validations import ModelValidator import pandas as pd def age_young ( df : pd . DataFrame ) -> pd . DataFrame : return df . loc [ df [ \"Age\" ] < 4 , :] def sex_female ( df : pd . DataFrame ) -> pd . DataFrame : return df . loc [ df [ \"Sex\" ] == \"female\" , :] def sex_male ( df : pd . DataFrame ) -> pd . DataFrame : return df . loc [ df [ \"Sex\" ] == \"male\" , :] slicing_functions = { \"children\" : age_young , \"female\" : sex_female , \"male\" : sex_male } model_validator = ModelValidator ( data = data_context , model = model , slicing_functions = slicing_functions ) validations = [ model_validator . validate_test_performance_against_dummy ( metric = \"accuracy\" , strategy = \"stratified\" , data_slice = \"female\" ) ]","title":"Metrics"},{"location":"metrics/#metrics-and-scoring-functions","text":"Many validations in the ModelValidator require the computation of a metric to validate. It is good practice to recompute these metrics outside of your training pipeline to avoid errors in passing on pre-computed metrics. As performance computation can be expensive in compute, a given metric is calculated once on a given dataset, and stored in the ModelValidator for use by any following validations. The dataset can be any datasets present in the DataContext , or any slices of these datasets. Example For example, here the ModelValidator computes recall on the test set for the first validation, and then uses this stored value in the second validation. from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) validations = [ model_validator . validate_test_performance_against_dummy ( metric = \"recall\" , strategy = \"stratified\" ) model_validator . validate_performance_between_train_and_test ( metric = \"recall\" , threshold = 0.3 ) ] print ( model_validator . performances ) # all performance values are stored in the performances attribute","title":"Metrics and scoring functions"},{"location":"metrics/#1-scikit-learn-scoring-functions","text":"The ModelValidator makes use of sklearn.metrics to compute model performance on a given dataset. The metric is fed into the validation as a string (as above), to use any of sklearn's scorers. Note List available scorers by running: import sklearn.metrics print ( sklearn . metrics . SCORERS )","title":"1. Scikit-learn scoring functions"},{"location":"metrics/#2-custom-scoring-functions","text":"To create your own scoring functions, scikit-learn provides a make_scorer function. In the case of an error metric, where a lower value is better, make use of the greater_is_better=False argument in order to stay consistent with comparison of metrics in performance validations. Example Here is an example of a custom scoring function being fed into the ModelValidator and used in a performance validation: from trubrics.validations import ModelValidator from sklearn.metrics import accuracy_score , make_scorer def custom_binary_error ( y_true , y_pred ): \"\"\" An example of an error metric for binary classification. \"\"\" accuracy = accuracy_score ( y_true , y_pred ) return round ( 1 - accuracy , 3 ) custom_score = make_scorer ( custom_binary_error , greater_is_better = False ) custom_scorers = { \"my_custom_loss\" : custom_score } model_validator = ModelValidator ( data = data_context , model = model , custom_scorers = custom_scorers ) validations = [ model_validator . validate_test_performance_against_dummy ( metric = \"my_custom_loss\" , strategy = \"stratified\" ) ]","title":"2. Custom scoring functions"},{"location":"metrics/#3-data-slicing-functions","text":"It is often necessary to compute performance on specific splits of a dataset, in order to test for model performance bias for example. Data slicing functions can also be fed into the ModelValidator , and used in validations in the same way that metrics are. Each function must take a single pandas dataframe argument, and return the filtered pandas dataframe. Example Here is an example of a series of data slicing functions being fed into the ModelValidator and used in a validation: from trubrics.validations import ModelValidator import pandas as pd def age_young ( df : pd . DataFrame ) -> pd . DataFrame : return df . loc [ df [ \"Age\" ] < 4 , :] def sex_female ( df : pd . DataFrame ) -> pd . DataFrame : return df . loc [ df [ \"Sex\" ] == \"female\" , :] def sex_male ( df : pd . DataFrame ) -> pd . DataFrame : return df . loc [ df [ \"Sex\" ] == \"male\" , :] slicing_functions = { \"children\" : age_young , \"female\" : sex_female , \"male\" : sex_male } model_validator = ModelValidator ( data = data_context , model = model , slicing_functions = slicing_functions ) validations = [ model_validator . validate_test_performance_against_dummy ( metric = \"accuracy\" , strategy = \"stratified\" , data_slice = \"female\" ) ]","title":"3. Data slicing functions"},{"location":"models/","text":"Compatible models with the ModelValidator 1. Load a scikit-learn model Trubrics integrates natively with the scikit-learn API , meaning any sklearn model may be inserted directly into a ModelValidator to create validations. This also means that we encourage you to use scikit's Pipeline object to include all model processing transformations in a single object. 2. Load your own model This example shows how we can wrap any custom model built with python to be used by the ModelValidator. A custom model is built with a python class that must contain: an attribute named _estimator_type (see attributes bellow). a predict() method with a pandas dataframe input argument, that returns a pandas series / numpy array of predict values (see source code below). Attributes: Name Type Description _estimator_type the estimator type can either be 'classifier' or 'regressor'. Example rule_based = RuleBasedModel () from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = rule_based ) Source code in tests/test_validations/test_custom_model.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class RuleBasedModel : \"\"\" This example shows how we can wrap any custom model built with python to be used by the ModelValidator. Tip: A custom model is built with a python class that must contain: - an attribute named _estimator_type (see attributes bellow). - a predict() method with a pandas dataframe input argument, that returns a pandas series / numpy array of predict values (see source code below). Attributes: _estimator_type: the estimator type can either be 'classifier' or 'regressor'. Example: ```py rule_based = RuleBasedModel() from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=rule_based) ``` \"\"\" def __init__ ( self ): self . _estimator_type = \"classifier\" def predict ( self , df ): \"\"\"Rule based predict classification function to determine a class based on single feature.\"\"\" df [ \"Survived\" ] = df [ \"Age\" ] . apply ( lambda x : 0 if x > 40 else 1 ) return df [ \"Survived\" ]","title":"Compatible models"},{"location":"models/#compatible-models-with-the-modelvalidator","text":"","title":"Compatible models with the ModelValidator"},{"location":"models/#1-load-a-scikit-learn-model","text":"Trubrics integrates natively with the scikit-learn API , meaning any sklearn model may be inserted directly into a ModelValidator to create validations. This also means that we encourage you to use scikit's Pipeline object to include all model processing transformations in a single object.","title":"1. Load a scikit-learn model"},{"location":"models/#2-load-your-own-model","text":"This example shows how we can wrap any custom model built with python to be used by the ModelValidator. A custom model is built with a python class that must contain: an attribute named _estimator_type (see attributes bellow). a predict() method with a pandas dataframe input argument, that returns a pandas series / numpy array of predict values (see source code below). Attributes: Name Type Description _estimator_type the estimator type can either be 'classifier' or 'regressor'. Example rule_based = RuleBasedModel () from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = rule_based ) Source code in tests/test_validations/test_custom_model.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class RuleBasedModel : \"\"\" This example shows how we can wrap any custom model built with python to be used by the ModelValidator. Tip: A custom model is built with a python class that must contain: - an attribute named _estimator_type (see attributes bellow). - a predict() method with a pandas dataframe input argument, that returns a pandas series / numpy array of predict values (see source code below). Attributes: _estimator_type: the estimator type can either be 'classifier' or 'regressor'. Example: ```py rule_based = RuleBasedModel() from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=rule_based) ``` \"\"\" def __init__ ( self ): self . _estimator_type = \"classifier\" def predict ( self , df ): \"\"\"Rule based predict classification function to determine a class based on single feature.\"\"\" df [ \"Survived\" ] = df [ \"Age\" ] . apply ( lambda x : 0 if x > 40 else 1 ) return df [ \"Survived\" ]","title":"2. Load your own model"},{"location":"save_trubric/","text":"Save validations as a trubric A Trubric is a list of validations that represents the gold standard a model must conform to. It also holds metadata about the model and DataContext that it has been run against. Once a Trubric has been saved (as .json), it can then be rerun against any other model and DataContext combination. 1. Save a Trubric Once an ensemble of validations have been built, the list can be input into the Trubric object and saved as a local .json file with the .save_local() method, and can be saved to the Trubrics platform with the .save_ui() method. Trubrics platform access Saving trubric runs to the Trubrics platform will allow full tracking of the evolution of validations, and sorting runs into projects. Teams within an organisation can push trubric runs to projects. Don't hesitate to get in touch with us here to gain access to the Trubrics platform for you and your team. Example from trubrics.validations import Trubric trubric = Trubric ( name = \"my_first_trubric\" , model_name = \"my_model\" , model_version = \"0.0.1\" , data_context_name = data_context . name , data_context_version = data_context . version , tags = [ \"master\" ], validations = validations , # a list of validations generated from the ModelValidator ) trubric . save_local () # optional path= parameter to specify a location to save the trubric trubric . save_ui () You are in charge of versioning your model and DataContext , and feeding in these values into the Trubric to keep track. Trubric object Bases: BaseModel Dataclass for a trubric, or set of validation points. Must be serialisable to .json. Attributes: Name Type Description name str trubric name model_name str model name model_version str model version data_context_name str data context name (from DataContext) data_context_version str data context version (from DataContext) metadata Optional [ Dict [ str , str ]] free textual metadata field validations List [ Validation ] list of validations (defined by Validation) Source code in trubrics/validations/dataclass.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 class Trubric ( BaseModel ): \"\"\" Dataclass for a trubric, or set of validation points. Must be serialisable to .json. Attributes: name: trubric name model_name: model name model_version: model version data_context_name: data context name (from DataContext) data_context_version: data context version (from DataContext) metadata: free textual metadata field validations: list of validations (defined by Validation) \"\"\" name : str model_name : str = \"my_model\" model_version : str = \"0.0.1\" data_context_name : str = \"my_data_context\" data_context_version : str = \"0.0.1\" validations : List [ Validation ] tags : List [ Optional [ str ]] = [] run_by : Optional [ Dict [ str , str ]] = None git_commit : Optional [ str ] = None metadata : Optional [ Dict [ str , str ]] = None timestamp : Optional [ int ] = None total_passed : Optional [ int ] = None total_passed_percent : Optional [ float ] = None class Config : extra = \"forbid\" def save_local ( self , path : Optional [ str ] = None ): self . _set_fields_on_save () if path is None : path = f \"./ { self . name } .json\" with open ( Path ( path ) . absolute (), \"w\" ) as file : file . write ( self . json ( indent = 4 )) logger . info ( f \"Trubric saved to { path } .\" ) def save_ui ( self ): trubrics_config = load_trubrics_config () if trubrics_config . email is None or trubrics_config . username is None or trubrics_config . password is None : raise TypeError ( \"Trubrics config not set. Run `trubrics init` to configure.\" ) auth = get_trubrics_auth_token ( trubrics_config . firebase_auth_api_url , trubrics_config . email , trubrics_config . password . get_secret_value () ) self . run_by = { \"email\" : trubrics_config . email , \"displayName\" : trubrics_config . username } self . _set_fields_on_save () res = add_document_to_project_subcollection ( auth , firestore_api_url = trubrics_config . firestore_api_url , project = trubrics_config . project , subcollection = \"trubrics\" , document_id = self . timestamp , document_json = self . json (), ) if \"error\" in res : error_msg = f \"Error in pushing trubric to the Trubrics UI: { res } \" logger . error ( error_msg ) raise Exception ( error_msg ) else : logger . info ( \"Trubric saved to the Trubrics UI.\" ) def _set_fields_on_save ( self ): self . total_passed = len ([ a for a in self . validations if a . outcome == \"pass\" ]) self . total_passed_percent = round ( 100 * self . total_passed / len ( self . validations ), 1 ) self . timestamp = int ( datetime . now () . timestamp ()) try : self . git_commit = Repo ( search_parent_directories = True ) . head . object . hexsha except InvalidGitRepositoryError : self . git_commit = None logger . warning ( \"Current directory is not a git repository. Run `trubrics run` inside a git repository to save the\" \" commit hash.\" ) 2. Run a saved Trubric Saved Trubric s can be run from the CLI or directly from a python environment (notebook, script, ipython kernel, etc): Example from trubrics.validations.run import TrubricRun trubric_run_context = TrubricRun ( data_context = data_context , model = model , trubric = Trubric . parse_file ( \"./my_first_trubric.json\" ), custom_validator = CustomValidator , custom_scorers = custom_scorers , slicing_functions = slicing_functions ) new_trubric = trubric_run_context . set_new_trubric () # save new trubric .json locally or to UI new_trubric . save_local () new_trubric . save_ui () TrubricRun object Bases: BaseModel The TrubricRun object to group all necessary code for a run. Load data and models from remote locations or locally for validation within a pipeline. Attributes: Name Type Description data_context DataContext a data context to validate a model on model Any a model to validate model_name str the name of the new model model_version str the version of the new model trubric Trubric a Trubric object listing all validations to execute metadata Optional [ Dict [ str , str ]] any new metadata to input to the Trubric tags List [ Optional [ str ]] any new tags for the trubric custom_validator Any an optional custom validator custom_scorers Optional [ Dict [ str , Any ]] an optional dict of custom scorers for computing custom metrics slicing_functions Optional [ Dict [ str , Any ]] an optional dict of slicing functions Source code in trubrics/validations/run.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 class TrubricRun ( BaseModel ): \"\"\"The TrubricRun object to group all necessary code for a run. Load data and models from remote locations or locally for validation within a pipeline. Attributes: data_context: a data context to validate a model on model: a model to validate model_name: the name of the new model model_version: the version of the new model trubric: a Trubric object listing all validations to execute metadata: any new metadata to input to the Trubric tags: any new tags for the trubric custom_validator: an optional custom validator custom_scorers: an optional dict of custom scorers for computing custom metrics slicing_functions: an optional dict of slicing functions \"\"\" data_context : DataContext model : Any model_name : str = \"new_model\" model_version : str = \"0.0.1\" trubric : Trubric metadata : Optional [ Dict [ str , str ]] = None tags : List [ Optional [ str ]] = [] custom_validator : Any = None custom_scorers : Optional [ Dict [ str , Any ]] = None slicing_functions : Optional [ Dict [ str , Any ]] = None @validator ( \"custom_validator\" ) def custom_validator_inherits_validator ( cls , val ): if issubclass ( val , ModelValidator ): return val raise TypeError ( \"Wrong type for 'custom_validator', must be subclass of ModelValidator.\" ) @validator ( \"custom_scorers\" ) def custom_scorer_is_make_scorer ( cls , val ): for scorer in val : if not issubclass ( type ( val [ scorer ]), _BaseScorer ): raise TypeError ( \"Each scorer must be subclass of scikit-learn's _BaseScorer.\" ) return val def generate_validations_from_trubric ( self ) -> Iterator [ Validation ]: if self . custom_validator is not None : model_validator = self . custom_validator ( data = self . data_context , model = self . model , custom_scorers = self . custom_scorers , slicing_functions = self . slicing_functions , ) else : model_validator = ModelValidator ( data = self . data_context , model = self . model , custom_scorers = self . custom_scorers , slicing_functions = self . slicing_functions , ) for validation in self . trubric . validations : args = validation . validation_kwargs [ \"args\" ] kwargs = validation . validation_kwargs [ \"kwargs\" ] try : validation_result = getattr ( model_validator , validation . validation_type )( * args , ** kwargs ) new_validation = validation . copy () new_validation . outcome = validation_result . outcome new_validation . result = validation_result . result yield new_validation except AttributeError : raise UnknownValidationError ( f \"The validation ' { validation . validation_type } ' does not appear to belong to a validator.\" \" Try adding the object that generated the validation to the 'custom_validator' parameter.\" ) def set_new_trubric ( self ) -> Trubric : all_validation_results = self . generate_validations_from_trubric () validations = [] for validation_result in all_validation_results : validations . append ( validation_result ) message_start = f \" { validation_result . validation_type } [ { validation_result . severity . upper () } ]\" completed_dots = f \"[grey82] { ( 100 - len ( message_start )) * '.' } [grey82]\" message_end = ( f \"[bold { 'green' if validation_result . outcome == 'pass' else 'red' } ] { validation_result . outcome . upper () } \" ) rprint ( message_start + completed_dots + message_end ) return Trubric ( name = self . trubric . name , model_name = self . model_name , model_version = self . model_version , data_context_name = self . data_context . name , data_context_version = self . data_context . version , metadata = self . metadata , tags = self . tags , validations = validations , )","title":"Save validations as a trubric"},{"location":"save_trubric/#save-validations-as-a-trubric","text":"A Trubric is a list of validations that represents the gold standard a model must conform to. It also holds metadata about the model and DataContext that it has been run against. Once a Trubric has been saved (as .json), it can then be rerun against any other model and DataContext combination.","title":"Save validations as a trubric"},{"location":"save_trubric/#1-save-a-trubric","text":"Once an ensemble of validations have been built, the list can be input into the Trubric object and saved as a local .json file with the .save_local() method, and can be saved to the Trubrics platform with the .save_ui() method. Trubrics platform access Saving trubric runs to the Trubrics platform will allow full tracking of the evolution of validations, and sorting runs into projects. Teams within an organisation can push trubric runs to projects. Don't hesitate to get in touch with us here to gain access to the Trubrics platform for you and your team. Example from trubrics.validations import Trubric trubric = Trubric ( name = \"my_first_trubric\" , model_name = \"my_model\" , model_version = \"0.0.1\" , data_context_name = data_context . name , data_context_version = data_context . version , tags = [ \"master\" ], validations = validations , # a list of validations generated from the ModelValidator ) trubric . save_local () # optional path= parameter to specify a location to save the trubric trubric . save_ui () You are in charge of versioning your model and DataContext , and feeding in these values into the Trubric to keep track. Trubric object Bases: BaseModel Dataclass for a trubric, or set of validation points. Must be serialisable to .json. Attributes: Name Type Description name str trubric name model_name str model name model_version str model version data_context_name str data context name (from DataContext) data_context_version str data context version (from DataContext) metadata Optional [ Dict [ str , str ]] free textual metadata field validations List [ Validation ] list of validations (defined by Validation) Source code in trubrics/validations/dataclass.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 class Trubric ( BaseModel ): \"\"\" Dataclass for a trubric, or set of validation points. Must be serialisable to .json. Attributes: name: trubric name model_name: model name model_version: model version data_context_name: data context name (from DataContext) data_context_version: data context version (from DataContext) metadata: free textual metadata field validations: list of validations (defined by Validation) \"\"\" name : str model_name : str = \"my_model\" model_version : str = \"0.0.1\" data_context_name : str = \"my_data_context\" data_context_version : str = \"0.0.1\" validations : List [ Validation ] tags : List [ Optional [ str ]] = [] run_by : Optional [ Dict [ str , str ]] = None git_commit : Optional [ str ] = None metadata : Optional [ Dict [ str , str ]] = None timestamp : Optional [ int ] = None total_passed : Optional [ int ] = None total_passed_percent : Optional [ float ] = None class Config : extra = \"forbid\" def save_local ( self , path : Optional [ str ] = None ): self . _set_fields_on_save () if path is None : path = f \"./ { self . name } .json\" with open ( Path ( path ) . absolute (), \"w\" ) as file : file . write ( self . json ( indent = 4 )) logger . info ( f \"Trubric saved to { path } .\" ) def save_ui ( self ): trubrics_config = load_trubrics_config () if trubrics_config . email is None or trubrics_config . username is None or trubrics_config . password is None : raise TypeError ( \"Trubrics config not set. Run `trubrics init` to configure.\" ) auth = get_trubrics_auth_token ( trubrics_config . firebase_auth_api_url , trubrics_config . email , trubrics_config . password . get_secret_value () ) self . run_by = { \"email\" : trubrics_config . email , \"displayName\" : trubrics_config . username } self . _set_fields_on_save () res = add_document_to_project_subcollection ( auth , firestore_api_url = trubrics_config . firestore_api_url , project = trubrics_config . project , subcollection = \"trubrics\" , document_id = self . timestamp , document_json = self . json (), ) if \"error\" in res : error_msg = f \"Error in pushing trubric to the Trubrics UI: { res } \" logger . error ( error_msg ) raise Exception ( error_msg ) else : logger . info ( \"Trubric saved to the Trubrics UI.\" ) def _set_fields_on_save ( self ): self . total_passed = len ([ a for a in self . validations if a . outcome == \"pass\" ]) self . total_passed_percent = round ( 100 * self . total_passed / len ( self . validations ), 1 ) self . timestamp = int ( datetime . now () . timestamp ()) try : self . git_commit = Repo ( search_parent_directories = True ) . head . object . hexsha except InvalidGitRepositoryError : self . git_commit = None logger . warning ( \"Current directory is not a git repository. Run `trubrics run` inside a git repository to save the\" \" commit hash.\" )","title":"1. Save a Trubric"},{"location":"save_trubric/#2-run-a-saved-trubric","text":"Saved Trubric s can be run from the CLI or directly from a python environment (notebook, script, ipython kernel, etc): Example from trubrics.validations.run import TrubricRun trubric_run_context = TrubricRun ( data_context = data_context , model = model , trubric = Trubric . parse_file ( \"./my_first_trubric.json\" ), custom_validator = CustomValidator , custom_scorers = custom_scorers , slicing_functions = slicing_functions ) new_trubric = trubric_run_context . set_new_trubric () # save new trubric .json locally or to UI new_trubric . save_local () new_trubric . save_ui () TrubricRun object Bases: BaseModel The TrubricRun object to group all necessary code for a run. Load data and models from remote locations or locally for validation within a pipeline. Attributes: Name Type Description data_context DataContext a data context to validate a model on model Any a model to validate model_name str the name of the new model model_version str the version of the new model trubric Trubric a Trubric object listing all validations to execute metadata Optional [ Dict [ str , str ]] any new metadata to input to the Trubric tags List [ Optional [ str ]] any new tags for the trubric custom_validator Any an optional custom validator custom_scorers Optional [ Dict [ str , Any ]] an optional dict of custom scorers for computing custom metrics slicing_functions Optional [ Dict [ str , Any ]] an optional dict of slicing functions Source code in trubrics/validations/run.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 class TrubricRun ( BaseModel ): \"\"\"The TrubricRun object to group all necessary code for a run. Load data and models from remote locations or locally for validation within a pipeline. Attributes: data_context: a data context to validate a model on model: a model to validate model_name: the name of the new model model_version: the version of the new model trubric: a Trubric object listing all validations to execute metadata: any new metadata to input to the Trubric tags: any new tags for the trubric custom_validator: an optional custom validator custom_scorers: an optional dict of custom scorers for computing custom metrics slicing_functions: an optional dict of slicing functions \"\"\" data_context : DataContext model : Any model_name : str = \"new_model\" model_version : str = \"0.0.1\" trubric : Trubric metadata : Optional [ Dict [ str , str ]] = None tags : List [ Optional [ str ]] = [] custom_validator : Any = None custom_scorers : Optional [ Dict [ str , Any ]] = None slicing_functions : Optional [ Dict [ str , Any ]] = None @validator ( \"custom_validator\" ) def custom_validator_inherits_validator ( cls , val ): if issubclass ( val , ModelValidator ): return val raise TypeError ( \"Wrong type for 'custom_validator', must be subclass of ModelValidator.\" ) @validator ( \"custom_scorers\" ) def custom_scorer_is_make_scorer ( cls , val ): for scorer in val : if not issubclass ( type ( val [ scorer ]), _BaseScorer ): raise TypeError ( \"Each scorer must be subclass of scikit-learn's _BaseScorer.\" ) return val def generate_validations_from_trubric ( self ) -> Iterator [ Validation ]: if self . custom_validator is not None : model_validator = self . custom_validator ( data = self . data_context , model = self . model , custom_scorers = self . custom_scorers , slicing_functions = self . slicing_functions , ) else : model_validator = ModelValidator ( data = self . data_context , model = self . model , custom_scorers = self . custom_scorers , slicing_functions = self . slicing_functions , ) for validation in self . trubric . validations : args = validation . validation_kwargs [ \"args\" ] kwargs = validation . validation_kwargs [ \"kwargs\" ] try : validation_result = getattr ( model_validator , validation . validation_type )( * args , ** kwargs ) new_validation = validation . copy () new_validation . outcome = validation_result . outcome new_validation . result = validation_result . result yield new_validation except AttributeError : raise UnknownValidationError ( f \"The validation ' { validation . validation_type } ' does not appear to belong to a validator.\" \" Try adding the object that generated the validation to the 'custom_validator' parameter.\" ) def set_new_trubric ( self ) -> Trubric : all_validation_results = self . generate_validations_from_trubric () validations = [] for validation_result in all_validation_results : validations . append ( validation_result ) message_start = f \" { validation_result . validation_type } [ { validation_result . severity . upper () } ]\" completed_dots = f \"[grey82] { ( 100 - len ( message_start )) * '.' } [grey82]\" message_end = ( f \"[bold { 'green' if validation_result . outcome == 'pass' else 'red' } ] { validation_result . outcome . upper () } \" ) rprint ( message_start + completed_dots + message_end ) return Trubric ( name = self . trubric . name , model_name = self . model_name , model_version = self . model_version , data_context_name = self . data_context . name , data_context_version = self . data_context . version , metadata = self . metadata , tags = self . tags , validations = validations , )","title":"2. Run a saved Trubric"},{"location":"trubrics_cli/","text":"Running trubrics from the CLI Once you have built a trubric of validations, you will want to test different data / models against that trubric. This will help you to ensure safe deployment of newly trained models directly from CI/CD/CT pipelines. Trubrics platform access This will allow you and your team to track all trubric runs in projects, and to close feedback issues by linking to specific runs. Don't hesitate to get in touch with us here to gain access to the Trubrics platform for you and your team. Complete these three steps to run trubrics from the CLI tool: 1. Create python runner script Create a python file <trubric_run_file>.py that loads datasets / models to validate and holds the necessary code to run all validations. This file must contain a RUN_CONTEXT variable with a value of TrubricRun , as in the example below. It is this RUN_CONTEXT variable that is read into the CLI tool at runtime. Example of <trubric_run_file>.py import joblib from trubrics.validations import DataContext from trubrics.validations.run import TrubricRun RUN_CONTEXT = TrubricRun ( data_context = DataContext ( ... ), # new data context model = joblib . load ( ... ), # new model trubric = Trubric . parse_file ( ... ) ) TrubricRun Object Bases: BaseModel The TrubricRun object to group all necessary code for a run. Load data and models from remote locations or locally for validation within a pipeline. Attributes: Name Type Description data_context DataContext a data context to validate a model on model Any a model to validate model_name str the name of the new model model_version str the version of the new model trubric Trubric a Trubric object listing all validations to execute metadata Optional [ Dict [ str , str ]] any new metadata to input to the Trubric tags List [ Optional [ str ]] any new tags for the trubric custom_validator Any an optional custom validator custom_scorers Optional [ Dict [ str , Any ]] an optional dict of custom scorers for computing custom metrics slicing_functions Optional [ Dict [ str , Any ]] an optional dict of slicing functions Source code in trubrics/validations/run.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 class TrubricRun ( BaseModel ): \"\"\"The TrubricRun object to group all necessary code for a run. Load data and models from remote locations or locally for validation within a pipeline. Attributes: data_context: a data context to validate a model on model: a model to validate model_name: the name of the new model model_version: the version of the new model trubric: a Trubric object listing all validations to execute metadata: any new metadata to input to the Trubric tags: any new tags for the trubric custom_validator: an optional custom validator custom_scorers: an optional dict of custom scorers for computing custom metrics slicing_functions: an optional dict of slicing functions \"\"\" data_context : DataContext model : Any model_name : str = \"new_model\" model_version : str = \"0.0.1\" trubric : Trubric metadata : Optional [ Dict [ str , str ]] = None tags : List [ Optional [ str ]] = [] custom_validator : Any = None custom_scorers : Optional [ Dict [ str , Any ]] = None slicing_functions : Optional [ Dict [ str , Any ]] = None @validator ( \"custom_validator\" ) def custom_validator_inherits_validator ( cls , val ): if issubclass ( val , ModelValidator ): return val raise TypeError ( \"Wrong type for 'custom_validator', must be subclass of ModelValidator.\" ) @validator ( \"custom_scorers\" ) def custom_scorer_is_make_scorer ( cls , val ): for scorer in val : if not issubclass ( type ( val [ scorer ]), _BaseScorer ): raise TypeError ( \"Each scorer must be subclass of scikit-learn's _BaseScorer.\" ) return val def generate_validations_from_trubric ( self ) -> Iterator [ Validation ]: if self . custom_validator is not None : model_validator = self . custom_validator ( data = self . data_context , model = self . model , custom_scorers = self . custom_scorers , slicing_functions = self . slicing_functions , ) else : model_validator = ModelValidator ( data = self . data_context , model = self . model , custom_scorers = self . custom_scorers , slicing_functions = self . slicing_functions , ) for validation in self . trubric . validations : args = validation . validation_kwargs [ \"args\" ] kwargs = validation . validation_kwargs [ \"kwargs\" ] try : validation_result = getattr ( model_validator , validation . validation_type )( * args , ** kwargs ) new_validation = validation . copy () new_validation . outcome = validation_result . outcome new_validation . result = validation_result . result yield new_validation except AttributeError : raise UnknownValidationError ( f \"The validation ' { validation . validation_type } ' does not appear to belong to a validator.\" \" Try adding the object that generated the validation to the 'custom_validator' parameter.\" ) def set_new_trubric ( self ) -> Trubric : all_validation_results = self . generate_validations_from_trubric () validations = [] for validation_result in all_validation_results : validations . append ( validation_result ) message_start = f \" { validation_result . validation_type } [ { validation_result . severity . upper () } ]\" completed_dots = f \"[grey82] { ( 100 - len ( message_start )) * '.' } [grey82]\" message_end = ( f \"[bold { 'green' if validation_result . outcome == 'pass' else 'red' } ] { validation_result . outcome . upper () } \" ) rprint ( message_start + completed_dots + message_end ) return Trubric ( name = self . trubric . name , model_name = self . model_name , model_version = self . model_version , data_context_name = self . data_context . name , data_context_version = self . data_context . version , metadata = self . metadata , tags = self . tags , validations = validations , ) 2. Connect to the Trubrics platform with trubrics init Initialise a run config in the terminal to save a ~/.trubrics_config.json file to your user's root directory. This config file holds credentials and connectivity for logging any data to the Trubrics platform. Be guided by the CLI prompts by running: Initialises the environment and authenticates with a Trubrics platform account. Parameters: Name Type Description Default api_key Optional [ str ] optional firebase api key None project_id Optional [ str ] optional firebase project ID None is_trubrics_user bool boolean of whether the user has a Trubrics account typer.Option(False, prompt='Do you already have an account with Trubrics?') Source code in trubrics/cli/main.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 @app . command () def init ( api_key : Optional [ str ] = None , project_id : Optional [ str ] = None , is_trubrics_user : bool = typer . Option ( False , prompt = \"Do you already have an account with Trubrics?\" ), ): \"\"\"Initialises the environment and authenticates with a Trubrics platform account. Args: api_key: optional firebase api key project_id: optional firebase project ID is_trubrics_user: boolean of whether the user has a Trubrics account \"\"\" if api_key or project_id : if not api_key or not project_id : raise Exception ( \"API key and project_id are both required to change project.\" ) defaults = TrubricsDefaults ( firebase_api_key = api_key , firebase_project_id = project_id ) else : defaults = TrubricsDefaults () if is_trubrics_user : email = typer . prompt ( \"Enter your user email\" ) password = typer . prompt ( \"Enter your user password\" , hide_input = True ) with Progress ( SpinnerColumn (), TextColumn ( \"[progress.description] {task.description} \" ), transient = True , ) as progress : progress . add_task ( description = \"Authenticating user...\" , total = None ) firebase_auth_api_url = get_trubrics_firebase_auth_api_url ( defaults . firebase_api_key ) auth = get_trubrics_auth_token ( firebase_auth_api_url , email , password ) if \"error\" in auth : rprint ( f \"Error in login email ' { email } ' to the Trubrics UI: { auth [ 'error' ] } \" ) raise typer . Abort () else : firestore_api_url = get_trubrics_firestore_api_url ( auth , defaults . firebase_project_id ) projects = list_projects_in_organisation ( firestore_api_url = firestore_api_url , auth = auth ) rprint ( f \" \\n [bold yellow]Welcome { auth [ 'displayName' ] } [bold yellow] :sunglasses: \\n \" ) if len ( projects ) > 0 : for index , project in enumerate ( projects ): rprint ( f \"[bold green][ { index } ][/bold green] [green] { project } [/green]\" ) project_num = typer . prompt ( \"Select your project (e.g. 0)\" ) project_int = int ( project_num ) if project_int not in list ( range ( len ( projects ))): message = typer . style ( f \"Project [ { project_num } ] not recognised.\" \"Please indicate an integer referring to one of the project names\" \" above.\" , fg = typer . colors . RED , bold = True , ) typer . echo ( message ) raise typer . Abort () else : project_name = projects [ project_int ] else : message = typer . style ( f \"Organisation ' { firestore_api_url . split ( '/' )[ - 1 ] } ' has no projects created.\" \" Navigate to the Trubrics UI to add a project.\" , fg = typer . colors . RED , bold = True , ) typer . echo ( message ) raise typer . Abort () trubrics_config = TrubricsConfig ( firebase_auth_api_url = firebase_auth_api_url , firestore_api_url = firestore_api_url , username = auth [ \"displayName\" ], email = email , password = password , project = project_name , # type: ignore ) trubrics_config . save () typer . echo ( typer . style ( \"Successful authentication with configuration:\" , fg = typer . colors . GREEN , bold = True )) rprint ( trubrics_config . dict ()) rprint () rprint ( \"[bold green]You can now push trubrics and feedback to the Trubrics platform:\" f \" \\n { defaults . trubrics_url } [bold green] \\n \" ) else : rprint ( \"[bold orange_red1]Sign up here to get access to the Trubrics platform:\" f \" \\n\\n { defaults . demo_sign_up_url } [bold orange_red1] \\n \" ) 3. Run validations and save locally or to Trubrics Run all validations from the terminal. This command also saves a new trubric .json with each validation's new outcome and results to a given path. Either save this trubric locally with --no-save-ui or to the Trubrics platform with --save-ui . Run trubric with titanic example You can test the trubrics run command with our titanic example model on 5 saved validations. Use --save-ui to save this example trubric to the Trubrics platform. (venv) $ trubrics run --no-save-ui \\ --run-context-path titanic-example-trubric \\ --trubric-output-file-path \"my_trubric.json\" Or be guided by the CLI prompts by running: Runs an example trubric (list of model validations) on the titanic dataset. Parameters: Name Type Description Default save_ui bool whether to save validations to the UI with in app user authentication typer.Option(False, prompt='Would you like to save your trubric to the UI?') run_context_path str path to the trubrics run context typer.Option(default='titanic-example-trubric', prompt='Enter the path to your trubric run .py file. Press enter for example') trubric_output_file_path str path to save your output trubric file typer.Option('./my_new_trubric.json', prompt='Enter a local path to save your output trubric file. Press enter for default path') Source code in trubrics/cli/main.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 @app . command () def run ( save_ui : bool = typer . Option ( False , prompt = \"Would you like to save your trubric to the UI?\" ), run_context_path : str = typer . Option ( default = \"titanic-example-trubric\" , prompt = \"Enter the path to your trubric run .py file. Press enter for example\" ), trubric_output_file_path : str = typer . Option ( \"./my_new_trubric.json\" , prompt = \"Enter a local path to save your output trubric file. Press enter for default path\" , ), ): \"\"\"Runs an example trubric (list of model validations) on the titanic dataset. Args: save_ui: whether to save validations to the UI with in app user authentication run_context_path: path to the trubrics run context trubric_output_file_path: path to save your output trubric file \"\"\" trubric_run_path = None if run_context_path != \"titanic-example-trubric\" : trubric_run_path = Path ( run_context_path ) . absolute () if not trubric_run_path . exists (): rprint ( f \"[red]Path ' { trubric_run_path } ' not found.[red]\" ) raise typer . Abort () rc = validate_trubric_run_context ( str ( trubric_run_path )) . RUN_CONTEXT else : from trubrics.example.trubric_run import RUN_CONTEXT as rc # type: ignore rprint ( f \" \\n Running trubric from file ' { trubric_run_path or 'trubrics.example.trubric_run' } ' with model\" f \" ' { rc . model_name } ' and dataset ' { rc . data_context . name } '. \\n \" ) new_trubric = rc . set_new_trubric () if save_ui : trubrics_config = load_trubrics_config () . dict () if trubrics_config [ \"email\" ] is not None : new_trubric . save_ui () else : typer . echo ( typer . style ( \"ERROR: You must authenticate with the trubrics manager by running `trubrics init` to remotely save\" \" trubrics runs.\" , fg = typer . colors . RED , ) ) else : defaults = TrubricsDefaults () new_trubric . save_local ( trubric_output_file_path ) rprint ( \" \\n [bold orange_red1]Be sure to check out our docs to see how you can leverage the Trubrics platform.\" f \" \\n\\n { defaults . demo_sign_up_url } [bold orange_red1] \\n \" )","title":"The Trubrics CLI"},{"location":"trubrics_cli/#running-trubrics-from-the-cli","text":"Once you have built a trubric of validations, you will want to test different data / models against that trubric. This will help you to ensure safe deployment of newly trained models directly from CI/CD/CT pipelines. Trubrics platform access This will allow you and your team to track all trubric runs in projects, and to close feedback issues by linking to specific runs. Don't hesitate to get in touch with us here to gain access to the Trubrics platform for you and your team. Complete these three steps to run trubrics from the CLI tool:","title":"Running trubrics from the CLI"},{"location":"trubrics_cli/#1-create-python-runner-script","text":"Create a python file <trubric_run_file>.py that loads datasets / models to validate and holds the necessary code to run all validations. This file must contain a RUN_CONTEXT variable with a value of TrubricRun , as in the example below. It is this RUN_CONTEXT variable that is read into the CLI tool at runtime. Example of <trubric_run_file>.py import joblib from trubrics.validations import DataContext from trubrics.validations.run import TrubricRun RUN_CONTEXT = TrubricRun ( data_context = DataContext ( ... ), # new data context model = joblib . load ( ... ), # new model trubric = Trubric . parse_file ( ... ) )","title":"1. Create python runner script"},{"location":"trubrics_cli/#trubricrun-object","text":"Bases: BaseModel The TrubricRun object to group all necessary code for a run. Load data and models from remote locations or locally for validation within a pipeline. Attributes: Name Type Description data_context DataContext a data context to validate a model on model Any a model to validate model_name str the name of the new model model_version str the version of the new model trubric Trubric a Trubric object listing all validations to execute metadata Optional [ Dict [ str , str ]] any new metadata to input to the Trubric tags List [ Optional [ str ]] any new tags for the trubric custom_validator Any an optional custom validator custom_scorers Optional [ Dict [ str , Any ]] an optional dict of custom scorers for computing custom metrics slicing_functions Optional [ Dict [ str , Any ]] an optional dict of slicing functions Source code in trubrics/validations/run.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 class TrubricRun ( BaseModel ): \"\"\"The TrubricRun object to group all necessary code for a run. Load data and models from remote locations or locally for validation within a pipeline. Attributes: data_context: a data context to validate a model on model: a model to validate model_name: the name of the new model model_version: the version of the new model trubric: a Trubric object listing all validations to execute metadata: any new metadata to input to the Trubric tags: any new tags for the trubric custom_validator: an optional custom validator custom_scorers: an optional dict of custom scorers for computing custom metrics slicing_functions: an optional dict of slicing functions \"\"\" data_context : DataContext model : Any model_name : str = \"new_model\" model_version : str = \"0.0.1\" trubric : Trubric metadata : Optional [ Dict [ str , str ]] = None tags : List [ Optional [ str ]] = [] custom_validator : Any = None custom_scorers : Optional [ Dict [ str , Any ]] = None slicing_functions : Optional [ Dict [ str , Any ]] = None @validator ( \"custom_validator\" ) def custom_validator_inherits_validator ( cls , val ): if issubclass ( val , ModelValidator ): return val raise TypeError ( \"Wrong type for 'custom_validator', must be subclass of ModelValidator.\" ) @validator ( \"custom_scorers\" ) def custom_scorer_is_make_scorer ( cls , val ): for scorer in val : if not issubclass ( type ( val [ scorer ]), _BaseScorer ): raise TypeError ( \"Each scorer must be subclass of scikit-learn's _BaseScorer.\" ) return val def generate_validations_from_trubric ( self ) -> Iterator [ Validation ]: if self . custom_validator is not None : model_validator = self . custom_validator ( data = self . data_context , model = self . model , custom_scorers = self . custom_scorers , slicing_functions = self . slicing_functions , ) else : model_validator = ModelValidator ( data = self . data_context , model = self . model , custom_scorers = self . custom_scorers , slicing_functions = self . slicing_functions , ) for validation in self . trubric . validations : args = validation . validation_kwargs [ \"args\" ] kwargs = validation . validation_kwargs [ \"kwargs\" ] try : validation_result = getattr ( model_validator , validation . validation_type )( * args , ** kwargs ) new_validation = validation . copy () new_validation . outcome = validation_result . outcome new_validation . result = validation_result . result yield new_validation except AttributeError : raise UnknownValidationError ( f \"The validation ' { validation . validation_type } ' does not appear to belong to a validator.\" \" Try adding the object that generated the validation to the 'custom_validator' parameter.\" ) def set_new_trubric ( self ) -> Trubric : all_validation_results = self . generate_validations_from_trubric () validations = [] for validation_result in all_validation_results : validations . append ( validation_result ) message_start = f \" { validation_result . validation_type } [ { validation_result . severity . upper () } ]\" completed_dots = f \"[grey82] { ( 100 - len ( message_start )) * '.' } [grey82]\" message_end = ( f \"[bold { 'green' if validation_result . outcome == 'pass' else 'red' } ] { validation_result . outcome . upper () } \" ) rprint ( message_start + completed_dots + message_end ) return Trubric ( name = self . trubric . name , model_name = self . model_name , model_version = self . model_version , data_context_name = self . data_context . name , data_context_version = self . data_context . version , metadata = self . metadata , tags = self . tags , validations = validations , )","title":"TrubricRun Object"},{"location":"trubrics_cli/#2-connect-to-the-trubrics-platform-with-trubrics-init","text":"Initialise a run config in the terminal to save a ~/.trubrics_config.json file to your user's root directory. This config file holds credentials and connectivity for logging any data to the Trubrics platform. Be guided by the CLI prompts by running: Initialises the environment and authenticates with a Trubrics platform account. Parameters: Name Type Description Default api_key Optional [ str ] optional firebase api key None project_id Optional [ str ] optional firebase project ID None is_trubrics_user bool boolean of whether the user has a Trubrics account typer.Option(False, prompt='Do you already have an account with Trubrics?') Source code in trubrics/cli/main.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 @app . command () def init ( api_key : Optional [ str ] = None , project_id : Optional [ str ] = None , is_trubrics_user : bool = typer . Option ( False , prompt = \"Do you already have an account with Trubrics?\" ), ): \"\"\"Initialises the environment and authenticates with a Trubrics platform account. Args: api_key: optional firebase api key project_id: optional firebase project ID is_trubrics_user: boolean of whether the user has a Trubrics account \"\"\" if api_key or project_id : if not api_key or not project_id : raise Exception ( \"API key and project_id are both required to change project.\" ) defaults = TrubricsDefaults ( firebase_api_key = api_key , firebase_project_id = project_id ) else : defaults = TrubricsDefaults () if is_trubrics_user : email = typer . prompt ( \"Enter your user email\" ) password = typer . prompt ( \"Enter your user password\" , hide_input = True ) with Progress ( SpinnerColumn (), TextColumn ( \"[progress.description] {task.description} \" ), transient = True , ) as progress : progress . add_task ( description = \"Authenticating user...\" , total = None ) firebase_auth_api_url = get_trubrics_firebase_auth_api_url ( defaults . firebase_api_key ) auth = get_trubrics_auth_token ( firebase_auth_api_url , email , password ) if \"error\" in auth : rprint ( f \"Error in login email ' { email } ' to the Trubrics UI: { auth [ 'error' ] } \" ) raise typer . Abort () else : firestore_api_url = get_trubrics_firestore_api_url ( auth , defaults . firebase_project_id ) projects = list_projects_in_organisation ( firestore_api_url = firestore_api_url , auth = auth ) rprint ( f \" \\n [bold yellow]Welcome { auth [ 'displayName' ] } [bold yellow] :sunglasses: \\n \" ) if len ( projects ) > 0 : for index , project in enumerate ( projects ): rprint ( f \"[bold green][ { index } ][/bold green] [green] { project } [/green]\" ) project_num = typer . prompt ( \"Select your project (e.g. 0)\" ) project_int = int ( project_num ) if project_int not in list ( range ( len ( projects ))): message = typer . style ( f \"Project [ { project_num } ] not recognised.\" \"Please indicate an integer referring to one of the project names\" \" above.\" , fg = typer . colors . RED , bold = True , ) typer . echo ( message ) raise typer . Abort () else : project_name = projects [ project_int ] else : message = typer . style ( f \"Organisation ' { firestore_api_url . split ( '/' )[ - 1 ] } ' has no projects created.\" \" Navigate to the Trubrics UI to add a project.\" , fg = typer . colors . RED , bold = True , ) typer . echo ( message ) raise typer . Abort () trubrics_config = TrubricsConfig ( firebase_auth_api_url = firebase_auth_api_url , firestore_api_url = firestore_api_url , username = auth [ \"displayName\" ], email = email , password = password , project = project_name , # type: ignore ) trubrics_config . save () typer . echo ( typer . style ( \"Successful authentication with configuration:\" , fg = typer . colors . GREEN , bold = True )) rprint ( trubrics_config . dict ()) rprint () rprint ( \"[bold green]You can now push trubrics and feedback to the Trubrics platform:\" f \" \\n { defaults . trubrics_url } [bold green] \\n \" ) else : rprint ( \"[bold orange_red1]Sign up here to get access to the Trubrics platform:\" f \" \\n\\n { defaults . demo_sign_up_url } [bold orange_red1] \\n \" )","title":"2. Connect to the Trubrics platform with trubrics init"},{"location":"trubrics_cli/#3-run-validations-and-save-locally-or-to-trubrics","text":"Run all validations from the terminal. This command also saves a new trubric .json with each validation's new outcome and results to a given path. Either save this trubric locally with --no-save-ui or to the Trubrics platform with --save-ui . Run trubric with titanic example You can test the trubrics run command with our titanic example model on 5 saved validations. Use --save-ui to save this example trubric to the Trubrics platform. (venv) $ trubrics run --no-save-ui \\ --run-context-path titanic-example-trubric \\ --trubric-output-file-path \"my_trubric.json\" Or be guided by the CLI prompts by running: Runs an example trubric (list of model validations) on the titanic dataset. Parameters: Name Type Description Default save_ui bool whether to save validations to the UI with in app user authentication typer.Option(False, prompt='Would you like to save your trubric to the UI?') run_context_path str path to the trubrics run context typer.Option(default='titanic-example-trubric', prompt='Enter the path to your trubric run .py file. Press enter for example') trubric_output_file_path str path to save your output trubric file typer.Option('./my_new_trubric.json', prompt='Enter a local path to save your output trubric file. Press enter for default path') Source code in trubrics/cli/main.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 @app . command () def run ( save_ui : bool = typer . Option ( False , prompt = \"Would you like to save your trubric to the UI?\" ), run_context_path : str = typer . Option ( default = \"titanic-example-trubric\" , prompt = \"Enter the path to your trubric run .py file. Press enter for example\" ), trubric_output_file_path : str = typer . Option ( \"./my_new_trubric.json\" , prompt = \"Enter a local path to save your output trubric file. Press enter for default path\" , ), ): \"\"\"Runs an example trubric (list of model validations) on the titanic dataset. Args: save_ui: whether to save validations to the UI with in app user authentication run_context_path: path to the trubrics run context trubric_output_file_path: path to save your output trubric file \"\"\" trubric_run_path = None if run_context_path != \"titanic-example-trubric\" : trubric_run_path = Path ( run_context_path ) . absolute () if not trubric_run_path . exists (): rprint ( f \"[red]Path ' { trubric_run_path } ' not found.[red]\" ) raise typer . Abort () rc = validate_trubric_run_context ( str ( trubric_run_path )) . RUN_CONTEXT else : from trubrics.example.trubric_run import RUN_CONTEXT as rc # type: ignore rprint ( f \" \\n Running trubric from file ' { trubric_run_path or 'trubrics.example.trubric_run' } ' with model\" f \" ' { rc . model_name } ' and dataset ' { rc . data_context . name } '. \\n \" ) new_trubric = rc . set_new_trubric () if save_ui : trubrics_config = load_trubrics_config () . dict () if trubrics_config [ \"email\" ] is not None : new_trubric . save_ui () else : typer . echo ( typer . style ( \"ERROR: You must authenticate with the trubrics manager by running `trubrics init` to remotely save\" \" trubrics runs.\" , fg = typer . colors . RED , ) ) else : defaults = TrubricsDefaults () new_trubric . save_local ( trubric_output_file_path ) rprint ( \" \\n [bold orange_red1]Be sure to check out our docs to see how you can leverage the Trubrics platform.\" f \" \\n\\n { defaults . demo_sign_up_url } [bold orange_red1] \\n \" )","title":"3. Run validations and save locally or to Trubrics"},{"location":"validations/","text":"Out-of-the-box model validations The trubrics library comes with out-of-the-box validations that you can test against your models in a couple of lines of code. These validations are held in the ModelValidator object, that can be instantiated with a DataContext and a model . This object can also be used to build custom validations . Minimum Functionality Minimum functionality validation. Validates that a model correctly predicts all points in a given set of data. This dataset must be set in the minimum_functionality_data parameter of the DataContext. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_minimum_functionality () Parameters: Name Type Description Default severity Optional [ str ] severity of the validation. Can be either ['error', 'warning', 'experiment']. If None, defaults to 'error'. None Returns: Type Description True for success, false otherwise. With a results dictionary giving all data points that were not correctly predicted by the model. Source code in trubrics/validations/model/base.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 @validation_output def validate_minimum_functionality ( self , severity : Optional [ str ] = None ): \"\"\"**Minimum functionality validation.** Validates that a model correctly predicts all points in a given set of data. This dataset must be set in the `minimum_functionality_data` parameter of the DataContext. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_minimum_functionality() ``` Args: severity: severity of the validation. Can be either ['error', 'warning', 'experiment']. \\ If None, defaults to 'error'. Returns: True for success, false otherwise. With a results dictionary giving all data points that were not \\ correctly predicted by the model. \"\"\" return self . _validate_minimum_functionality () Minimum functionality validation for a range output. Validates that a model correctly predicts all points in a given set of data, within a range of values. This dataset must be set in the minimum_functionality_data parameter of the DataContext. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_minimum_functionality_in_range ( range_value = 0 , range_inclusive = True ) Parameters: Name Type Description Default range_value Union [ int , float ] a value that is added to and subtracted from the target value for a given prediction, to create a range of possible values that the prediction should fall between. 0 range_inclusive bool make range inclusive (x <= prediction <= y) or exclusive (x <= prediction <= y) True severity Optional [ str ] severity of the validation. Can be either ['error', 'warning', 'experiment']. If None, defaults to 'error'. None Returns: Type Description True for success, false otherwise. With a results dictionary giving all data points where the model's prediction did not fall between the range given. Source code in trubrics/validations/model/base.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 @validation_output def validate_minimum_functionality_in_range ( self , range_value : Union [ int , float ] = 0 , range_inclusive : bool = True , severity : Optional [ str ] = None ): \"\"\"**Minimum functionality validation for a range output.** Validates that a model correctly predicts all points in a given set of data, within a range of values. This dataset must be set in the `minimum_functionality_data` parameter of the DataContext. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_minimum_functionality_in_range( range_value=0, range_inclusive=True ) ``` Args: range_value: a value that is added to and subtracted from the target value for a given prediction, to create a range of possible values that the prediction should fall between. range_inclusive: make range inclusive (x <= prediction <= y) or exclusive (x <= prediction <= y) severity: severity of the validation. Can be either ['error', 'warning', 'experiment']. \\ If None, defaults to 'error'. Returns: True for success, false otherwise. With a results dictionary giving all data points where \\ the model's prediction did not fall between the range given. \"\"\" return self . _validate_minimum_functionality_in_range ( range_value , range_inclusive = range_inclusive ) Performance Performance validation versus a fixed threshold value. Compares performance of a model on any of the datasets in the DataContext to a hard coded threshold value. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_performance_against_threshold ( metric = \"recall\" , threshold = 0.8 ) Parameters: Name Type Description Default metric str performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object. required threshold float the performance threshold that the model must attain. required dataset str the name of a dataset from the DataContext {'testing_data', 'training_data'}. 'testing_data' data_slice Optional [ str ] the name of the data slice, specified in the slicing_functions parameter of ModelValidator. None severity Optional [ str ] severity of the validation. Can be either {'error', 'warning', 'experiment'}. If None, defaults to 'error'. None Returns: Type Description True for success, false otherwise. With a results dictionary giving the actual model performance calculated. Source code in trubrics/validations/model/base.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 @validation_output def validate_performance_against_threshold ( self , metric : str , threshold : float , dataset : str = \"testing_data\" , data_slice : Optional [ str ] = None , severity : Optional [ str ] = None , ): \"\"\"**Performance validation versus a fixed threshold value.** Compares performance of a model on any of the datasets in the DataContext to a hard coded threshold value. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_performance_against_threshold( metric=\"recall\", threshold=0.8 ) ``` Args: metric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a \\ custom scorer fed in when initialising the ModelValidator object. threshold: the performance threshold that the model must attain. dataset: the name of a dataset from the DataContext {'testing_data', 'training_data'}. data_slice: the name of the data slice, specified in the slicing_functions parameter of ModelValidator. severity: severity of the validation. Can be either {'error', 'warning', 'experiment'}. \\ If None, defaults to 'error'. Returns: True for success, false otherwise. With a results dictionary giving the actual model performance calculated. \"\"\" return self . _validate_performance_against_threshold ( metric , threshold , dataset , data_slice ) Performance validation of testing data versus a dummy baseline model. Trains a DummyClassifier / DummyRegressor from sklearn and compares performance against the model on the test set. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_test_performance_against_dummy ( metric = \"accuracy\" , strategy = \"stratified\" ) Parameters: Name Type Description Default metric str performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object. required strategy str strategy of scikit-learns dummy model. 'most_frequent' dummy_kwargs Optional [ dict ] kwargs to be passed to dummy model. None data_slice Optional [ str ] the name of the data slice, specified in the slicing_functions parameter of ModelValidator. None severity Optional [ str ] severity of the validation. Can be either ['error', 'warning', 'experiment']. If None, defaults to 'error'. None Returns: Type Description True for success, false otherwise. With a results dictionary giving the model's actual performance on the test set and the dummy model's performance. Source code in trubrics/validations/model/base.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 @validation_output def validate_test_performance_against_dummy ( self , metric : str , strategy : str = \"most_frequent\" , dummy_kwargs : Optional [ dict ] = None , data_slice : Optional [ str ] = None , severity : Optional [ str ] = None , ): \"\"\"**Performance validation of testing data versus a dummy baseline model.** Trains a DummyClassifier / DummyRegressor from \\ [sklearn](https://scikit-learn.org/stable/modules/classes.html?highlight=dummy#module-sklearn.dummy)\\ and compares performance against the model on the test set. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_test_performance_against_dummy( metric=\"accuracy\", strategy=\"stratified\" ) ``` Args: metric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a \\ custom scorer fed in when initialising the ModelValidator object. strategy: strategy of scikit-learns dummy model. dummy_kwargs: kwargs to be passed to dummy model. data_slice: the name of the data slice, specified in the slicing_functions parameter of ModelValidator. severity: severity of the validation. Can be either ['error', 'warning', 'experiment']. \\ If None, defaults to 'error'. Returns: True for success, false otherwise. With a results dictionary giving the model's \\ actual performance on the test set and the dummy model's performance. \"\"\" return self . _validate_test_performance_against_dummy ( metric , strategy , dummy_kwargs , data_slice ) Performance validation comparing training and test data scores. Scores the test set and the train set in the DataContext, and validates whether the test score is inferior to but also within a certain range of the train score. Can be used to validate for overfitting on the training set. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_performance_between_train_and_test ( metric = \"recall\" , threshold = 0.3 ) Parameters: Name Type Description Default metric str performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object. required threshold Union [ int , float ] a positive value representing the maximum allowable difference between the train and test score. required data_slice Optional [ str ] the name of the data slice, specified in the slicing_functions parameter of ModelValidator. None severity Optional [ str ] severity of the validation. Can be either ['error', 'warning', 'experiment']. If None, defaults to 'error'. None Returns: Type Description True for success, false otherwise. With a results dictionary giving the model's performance on test and train sets. Source code in trubrics/validations/model/base.py 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 @validation_output def validate_performance_between_train_and_test ( self , metric : str , threshold : Union [ int , float ], data_slice : Optional [ str ] = None , severity : Optional [ str ] = None , ): \"\"\"**Performance validation comparing training and test data scores.** Scores the test set and the train set in the DataContext, and validates whether the test score is \\ inferior to but also within a certain range of the train score. Can be used to validate for overfitting on the training set. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_performance_between_train_and_test( metric=\"recall\", threshold=0.3 ) ``` Args: metric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a \\ custom scorer fed in when initialising the ModelValidator object. threshold: a positive value representing the maximum allowable difference between the train and \\ test score. data_slice: the name of the data slice, specified in the slicing_functions parameter of ModelValidator. severity: severity of the validation. Can be either ['error', 'warning', 'experiment']. \\ If None, defaults to 'error'. Returns: True for success, false otherwise. With a results dictionary giving the model's \\ performance on test and train sets. \"\"\" return self . _validate_performance_between_train_and_test ( metric , threshold , data_slice ) Performance validation comparing data slices. Validates that a list of model performances on different data slices from a given dataset has a lower standard deviation than a given threshold value. Example from trubrics.validations import ModelValidator slicing_functions = { \"female\" : lambda x : x [ x [ \"Sex\" ] == \"female\" ], \"male\" : lambda x : x [ x [ \"Sex\" ] == \"male\" ]} model_validator = ModelValidator ( data = data_context , model = model , slicing_functions = slicing_functions ) model_validator . validate_performance_std_across_slices ( metric = \"recall\" , dataset = \"training_data\" , data_slices = [ \"male\" , \"female\" ], std_threshold = 0.05 , include_global_performance = True ) Parameters: Name Type Description Default metric str performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object. required data_slices List [ str ] a list of of data slices, specified in the slicing_functions parameter of ModelValidator. required std_threshold float the standard deviation threshold that must be superior to the standard deviation of all data slice performances. required dataset str the name of a dataset from the DataContext {'testing_data', 'training_data'}. 'testing_data' include_global_performance bool whether or not to include the dataset global performance in the list. False severity Optional [ str ] severity of the validation. Can be either {'error', 'warning', 'experiment'}. If None, defaults to 'error'. None Source code in trubrics/validations/model/base.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 @validation_output def validate_performance_std_across_slices ( self , metric : str , data_slices : List [ str ], std_threshold : float , dataset : str = \"testing_data\" , include_global_performance : bool = False , severity : Optional [ str ] = None , ): \"\"\"**Performance validation comparing data slices.** Validates that a list of model performances on different data slices from a given dataset has a lower standard deviation than a given threshold value. Example: ```py from trubrics.validations import ModelValidator slicing_functions = {\"female\": lambda x: x[x[\"Sex\"]==\"female\"], \"male\": lambda x: x[x[\"Sex\"]==\"male\"]} model_validator = ModelValidator(data=data_context, model=model, slicing_functions=slicing_functions) model_validator.validate_performance_std_across_slices( metric=\"recall\", dataset=\"training_data\", data_slices=[\"male\", \"female\"], std_threshold=0.05, include_global_performance=True ) ``` Args: metric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a \\ custom scorer fed in when initialising the ModelValidator object. data_slices: a list of of data slices, specified in the slicing_functions parameter of ModelValidator. std_threshold: the standard deviation threshold that must be superior to the standard deviation of all \\ data slice performances. dataset: the name of a dataset from the DataContext {'testing_data', 'training_data'}. include_global_performance: whether or not to include the dataset global performance in the list. severity: severity of the validation. Can be either {'error', 'warning', 'experiment'}. \\ If None, defaults to 'error'. \"\"\" return self . _validate_performance_std_across_slices ( metric , data_slices , std_threshold , dataset , include_global_performance ) Inference time Model inference time validation. Validate the model's inference time on a single data point from the test set. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_inference_time ( threshold = 0.04 , n_executions = 100 ) Parameters: Name Type Description Default threshold float number of seconds that the model inference time should be inferior to. required n_executions int number of executions of the .predict() method for a single data point. The inference time will be the mean of each of the n_executions. 100 Returns: Type Description True for success, false otherwise. With a results dictionary giving the model's average inference time (in seconds). Source code in trubrics/validations/model/base.py 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 @validation_output def validate_inference_time ( self , threshold : float , n_executions : int = 100 , severity : Optional [ str ] = None ): \"\"\"**Model inference time validation.** Validate the model's inference time on a single data point from the test set. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_inference_time(threshold=0.04, n_executions=100) ``` Args: threshold: number of seconds that the model inference time should be inferior to. n_executions: number of executions of the `.predict()` method for a single data point. \\ The inference time will be the mean of each of the n_executions. Returns: True for success, false otherwise. With a results dictionary giving the model's \\ average inference time (in seconds). \"\"\" return self . _validate_inference_time ( threshold , n_executions ) Feature Importance Feature importance calculation for the following validations is based on sklearn's permutation_importance . Feature importance validation for top n features. Validates that a given feature is in the top n most important features. For calculation of feature importance we are using sklearn's permutation_importance. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_feature_in_top_n_important_features ( dataset = \"testing_data\" , feature = \"feature_a\" , top_n_features = 2 , ) Parameters: Name Type Description Default feature str feature to assess. required top_n_features int the number of most important features that the named feature must be ranked in. E.g. if top_n_features=2, the feature must be within the top two most important features. required dataset str the name of a dataset from the DataContext to calculate feature importance on {'testing_data', 'training_data'}. 'testing_data' permutation_kwargs Optional [ Dict [ str , Any ]] kwargs to pass into the sklearn.inspection.permutation_importance function. None Returns: Type Description True for success, false otherwise. With a results dictionary giving the actual feature importance ranking. Source code in trubrics/validations/model/base.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 @validation_output def validate_feature_in_top_n_important_features ( self , feature : str , top_n_features : int , dataset : str = \"testing_data\" , permutation_kwargs : Optional [ Dict [ str , Any ]] = None , severity : Optional [ str ] = None , ): \"\"\"**Feature importance validation for top n features.** Validates that a given feature is in the top n most important features. For calculation of feature \\ importance we are using sklearn's permutation_importance. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_feature_in_top_n_important_features( dataset=\"testing_data\", feature=\"feature_a\", top_n_features=2, ) ``` Args: feature: feature to assess. top_n_features: the number of most important features that the named feature must be ranked in. E.g. if top_n_features=2, the feature must be within the top two most important features. dataset: the name of a dataset from the DataContext to calculate feature importance on \\ {'testing_data', 'training_data'}. permutation_kwargs: kwargs to pass into the sklearn.inspection.permutation_importance function. Returns: True for success, false otherwise. With a results dictionary giving the actual feature importance ranking. \"\"\" return self . _validate_feature_in_top_n_important_features ( feature , top_n_features , dataset , permutation_kwargs ) Permutation feature importance validation between train and test sets. Validates that the ranking of top n features is the same for both test and train sets. For calculation of feature importance we are using sklearn's permutation_importance. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_feature_importance_between_train_and_test ( top_n_features = 1 ) Parameters: Name Type Description Default top_n_features Optional [ int ] the number of most important features to consider for comparison between train and test sets. E.g. if top_n_features=2, the train and test sets must have the same 2 most important features, in the same order. None permutation_kwargs Optional [ Dict [ str , Any ]] kwargs to pass into the sklearn.inspection.permutation_importance function. None Returns: Type Description True for success, false otherwise. With a results dictionary giving the actual feature importance ranking. Source code in trubrics/validations/model/base.py 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 @validation_output def validate_feature_importance_between_train_and_test ( self , top_n_features : Optional [ int ] = None , permutation_kwargs : Optional [ Dict [ str , Any ]] = None ): \"\"\"**Permutation feature importance validation between train and test sets.** Validates that the ranking of top n features is the same for both test and train sets. For calculation of \\ feature importance we are using sklearn's permutation_importance. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_feature_importance_between_train_and_test( top_n_features=1 ) ``` Args: top_n_features: the number of most important features to consider for comparison between train and test \\ sets. E.g. if top_n_features=2, the train and test sets must have the same 2 most \\ important features, in the same order. permutation_kwargs: kwargs to pass into the sklearn.inspection.permutation_importance function. Returns: True for success, false otherwise. With a results dictionary giving the actual feature importance ranking. \"\"\" return self . _validate_feature_importance_between_train_and_test ( top_n_features , permutation_kwargs )","title":"Out-of-the-box validations"},{"location":"validations/#out-of-the-box-model-validations","text":"The trubrics library comes with out-of-the-box validations that you can test against your models in a couple of lines of code. These validations are held in the ModelValidator object, that can be instantiated with a DataContext and a model . This object can also be used to build custom validations .","title":"Out-of-the-box model validations"},{"location":"validations/#minimum-functionality","text":"Minimum functionality validation. Validates that a model correctly predicts all points in a given set of data. This dataset must be set in the minimum_functionality_data parameter of the DataContext. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_minimum_functionality () Parameters: Name Type Description Default severity Optional [ str ] severity of the validation. Can be either ['error', 'warning', 'experiment']. If None, defaults to 'error'. None Returns: Type Description True for success, false otherwise. With a results dictionary giving all data points that were not correctly predicted by the model. Source code in trubrics/validations/model/base.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 @validation_output def validate_minimum_functionality ( self , severity : Optional [ str ] = None ): \"\"\"**Minimum functionality validation.** Validates that a model correctly predicts all points in a given set of data. This dataset must be set in the `minimum_functionality_data` parameter of the DataContext. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_minimum_functionality() ``` Args: severity: severity of the validation. Can be either ['error', 'warning', 'experiment']. \\ If None, defaults to 'error'. Returns: True for success, false otherwise. With a results dictionary giving all data points that were not \\ correctly predicted by the model. \"\"\" return self . _validate_minimum_functionality () Minimum functionality validation for a range output. Validates that a model correctly predicts all points in a given set of data, within a range of values. This dataset must be set in the minimum_functionality_data parameter of the DataContext. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_minimum_functionality_in_range ( range_value = 0 , range_inclusive = True ) Parameters: Name Type Description Default range_value Union [ int , float ] a value that is added to and subtracted from the target value for a given prediction, to create a range of possible values that the prediction should fall between. 0 range_inclusive bool make range inclusive (x <= prediction <= y) or exclusive (x <= prediction <= y) True severity Optional [ str ] severity of the validation. Can be either ['error', 'warning', 'experiment']. If None, defaults to 'error'. None Returns: Type Description True for success, false otherwise. With a results dictionary giving all data points where the model's prediction did not fall between the range given. Source code in trubrics/validations/model/base.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 @validation_output def validate_minimum_functionality_in_range ( self , range_value : Union [ int , float ] = 0 , range_inclusive : bool = True , severity : Optional [ str ] = None ): \"\"\"**Minimum functionality validation for a range output.** Validates that a model correctly predicts all points in a given set of data, within a range of values. This dataset must be set in the `minimum_functionality_data` parameter of the DataContext. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_minimum_functionality_in_range( range_value=0, range_inclusive=True ) ``` Args: range_value: a value that is added to and subtracted from the target value for a given prediction, to create a range of possible values that the prediction should fall between. range_inclusive: make range inclusive (x <= prediction <= y) or exclusive (x <= prediction <= y) severity: severity of the validation. Can be either ['error', 'warning', 'experiment']. \\ If None, defaults to 'error'. Returns: True for success, false otherwise. With a results dictionary giving all data points where \\ the model's prediction did not fall between the range given. \"\"\" return self . _validate_minimum_functionality_in_range ( range_value , range_inclusive = range_inclusive )","title":"Minimum Functionality"},{"location":"validations/#performance","text":"Performance validation versus a fixed threshold value. Compares performance of a model on any of the datasets in the DataContext to a hard coded threshold value. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_performance_against_threshold ( metric = \"recall\" , threshold = 0.8 ) Parameters: Name Type Description Default metric str performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object. required threshold float the performance threshold that the model must attain. required dataset str the name of a dataset from the DataContext {'testing_data', 'training_data'}. 'testing_data' data_slice Optional [ str ] the name of the data slice, specified in the slicing_functions parameter of ModelValidator. None severity Optional [ str ] severity of the validation. Can be either {'error', 'warning', 'experiment'}. If None, defaults to 'error'. None Returns: Type Description True for success, false otherwise. With a results dictionary giving the actual model performance calculated. Source code in trubrics/validations/model/base.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 @validation_output def validate_performance_against_threshold ( self , metric : str , threshold : float , dataset : str = \"testing_data\" , data_slice : Optional [ str ] = None , severity : Optional [ str ] = None , ): \"\"\"**Performance validation versus a fixed threshold value.** Compares performance of a model on any of the datasets in the DataContext to a hard coded threshold value. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_performance_against_threshold( metric=\"recall\", threshold=0.8 ) ``` Args: metric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a \\ custom scorer fed in when initialising the ModelValidator object. threshold: the performance threshold that the model must attain. dataset: the name of a dataset from the DataContext {'testing_data', 'training_data'}. data_slice: the name of the data slice, specified in the slicing_functions parameter of ModelValidator. severity: severity of the validation. Can be either {'error', 'warning', 'experiment'}. \\ If None, defaults to 'error'. Returns: True for success, false otherwise. With a results dictionary giving the actual model performance calculated. \"\"\" return self . _validate_performance_against_threshold ( metric , threshold , dataset , data_slice ) Performance validation of testing data versus a dummy baseline model. Trains a DummyClassifier / DummyRegressor from sklearn and compares performance against the model on the test set. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_test_performance_against_dummy ( metric = \"accuracy\" , strategy = \"stratified\" ) Parameters: Name Type Description Default metric str performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object. required strategy str strategy of scikit-learns dummy model. 'most_frequent' dummy_kwargs Optional [ dict ] kwargs to be passed to dummy model. None data_slice Optional [ str ] the name of the data slice, specified in the slicing_functions parameter of ModelValidator. None severity Optional [ str ] severity of the validation. Can be either ['error', 'warning', 'experiment']. If None, defaults to 'error'. None Returns: Type Description True for success, false otherwise. With a results dictionary giving the model's actual performance on the test set and the dummy model's performance. Source code in trubrics/validations/model/base.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 @validation_output def validate_test_performance_against_dummy ( self , metric : str , strategy : str = \"most_frequent\" , dummy_kwargs : Optional [ dict ] = None , data_slice : Optional [ str ] = None , severity : Optional [ str ] = None , ): \"\"\"**Performance validation of testing data versus a dummy baseline model.** Trains a DummyClassifier / DummyRegressor from \\ [sklearn](https://scikit-learn.org/stable/modules/classes.html?highlight=dummy#module-sklearn.dummy)\\ and compares performance against the model on the test set. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_test_performance_against_dummy( metric=\"accuracy\", strategy=\"stratified\" ) ``` Args: metric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a \\ custom scorer fed in when initialising the ModelValidator object. strategy: strategy of scikit-learns dummy model. dummy_kwargs: kwargs to be passed to dummy model. data_slice: the name of the data slice, specified in the slicing_functions parameter of ModelValidator. severity: severity of the validation. Can be either ['error', 'warning', 'experiment']. \\ If None, defaults to 'error'. Returns: True for success, false otherwise. With a results dictionary giving the model's \\ actual performance on the test set and the dummy model's performance. \"\"\" return self . _validate_test_performance_against_dummy ( metric , strategy , dummy_kwargs , data_slice ) Performance validation comparing training and test data scores. Scores the test set and the train set in the DataContext, and validates whether the test score is inferior to but also within a certain range of the train score. Can be used to validate for overfitting on the training set. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_performance_between_train_and_test ( metric = \"recall\" , threshold = 0.3 ) Parameters: Name Type Description Default metric str performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object. required threshold Union [ int , float ] a positive value representing the maximum allowable difference between the train and test score. required data_slice Optional [ str ] the name of the data slice, specified in the slicing_functions parameter of ModelValidator. None severity Optional [ str ] severity of the validation. Can be either ['error', 'warning', 'experiment']. If None, defaults to 'error'. None Returns: Type Description True for success, false otherwise. With a results dictionary giving the model's performance on test and train sets. Source code in trubrics/validations/model/base.py 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 @validation_output def validate_performance_between_train_and_test ( self , metric : str , threshold : Union [ int , float ], data_slice : Optional [ str ] = None , severity : Optional [ str ] = None , ): \"\"\"**Performance validation comparing training and test data scores.** Scores the test set and the train set in the DataContext, and validates whether the test score is \\ inferior to but also within a certain range of the train score. Can be used to validate for overfitting on the training set. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_performance_between_train_and_test( metric=\"recall\", threshold=0.3 ) ``` Args: metric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a \\ custom scorer fed in when initialising the ModelValidator object. threshold: a positive value representing the maximum allowable difference between the train and \\ test score. data_slice: the name of the data slice, specified in the slicing_functions parameter of ModelValidator. severity: severity of the validation. Can be either ['error', 'warning', 'experiment']. \\ If None, defaults to 'error'. Returns: True for success, false otherwise. With a results dictionary giving the model's \\ performance on test and train sets. \"\"\" return self . _validate_performance_between_train_and_test ( metric , threshold , data_slice ) Performance validation comparing data slices. Validates that a list of model performances on different data slices from a given dataset has a lower standard deviation than a given threshold value. Example from trubrics.validations import ModelValidator slicing_functions = { \"female\" : lambda x : x [ x [ \"Sex\" ] == \"female\" ], \"male\" : lambda x : x [ x [ \"Sex\" ] == \"male\" ]} model_validator = ModelValidator ( data = data_context , model = model , slicing_functions = slicing_functions ) model_validator . validate_performance_std_across_slices ( metric = \"recall\" , dataset = \"training_data\" , data_slices = [ \"male\" , \"female\" ], std_threshold = 0.05 , include_global_performance = True ) Parameters: Name Type Description Default metric str performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object. required data_slices List [ str ] a list of of data slices, specified in the slicing_functions parameter of ModelValidator. required std_threshold float the standard deviation threshold that must be superior to the standard deviation of all data slice performances. required dataset str the name of a dataset from the DataContext {'testing_data', 'training_data'}. 'testing_data' include_global_performance bool whether or not to include the dataset global performance in the list. False severity Optional [ str ] severity of the validation. Can be either {'error', 'warning', 'experiment'}. If None, defaults to 'error'. None Source code in trubrics/validations/model/base.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 @validation_output def validate_performance_std_across_slices ( self , metric : str , data_slices : List [ str ], std_threshold : float , dataset : str = \"testing_data\" , include_global_performance : bool = False , severity : Optional [ str ] = None , ): \"\"\"**Performance validation comparing data slices.** Validates that a list of model performances on different data slices from a given dataset has a lower standard deviation than a given threshold value. Example: ```py from trubrics.validations import ModelValidator slicing_functions = {\"female\": lambda x: x[x[\"Sex\"]==\"female\"], \"male\": lambda x: x[x[\"Sex\"]==\"male\"]} model_validator = ModelValidator(data=data_context, model=model, slicing_functions=slicing_functions) model_validator.validate_performance_std_across_slices( metric=\"recall\", dataset=\"training_data\", data_slices=[\"male\", \"female\"], std_threshold=0.05, include_global_performance=True ) ``` Args: metric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a \\ custom scorer fed in when initialising the ModelValidator object. data_slices: a list of of data slices, specified in the slicing_functions parameter of ModelValidator. std_threshold: the standard deviation threshold that must be superior to the standard deviation of all \\ data slice performances. dataset: the name of a dataset from the DataContext {'testing_data', 'training_data'}. include_global_performance: whether or not to include the dataset global performance in the list. severity: severity of the validation. Can be either {'error', 'warning', 'experiment'}. \\ If None, defaults to 'error'. \"\"\" return self . _validate_performance_std_across_slices ( metric , data_slices , std_threshold , dataset , include_global_performance )","title":"Performance"},{"location":"validations/#inference-time","text":"Model inference time validation. Validate the model's inference time on a single data point from the test set. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_inference_time ( threshold = 0.04 , n_executions = 100 ) Parameters: Name Type Description Default threshold float number of seconds that the model inference time should be inferior to. required n_executions int number of executions of the .predict() method for a single data point. The inference time will be the mean of each of the n_executions. 100 Returns: Type Description True for success, false otherwise. With a results dictionary giving the model's average inference time (in seconds). Source code in trubrics/validations/model/base.py 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 @validation_output def validate_inference_time ( self , threshold : float , n_executions : int = 100 , severity : Optional [ str ] = None ): \"\"\"**Model inference time validation.** Validate the model's inference time on a single data point from the test set. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_inference_time(threshold=0.04, n_executions=100) ``` Args: threshold: number of seconds that the model inference time should be inferior to. n_executions: number of executions of the `.predict()` method for a single data point. \\ The inference time will be the mean of each of the n_executions. Returns: True for success, false otherwise. With a results dictionary giving the model's \\ average inference time (in seconds). \"\"\" return self . _validate_inference_time ( threshold , n_executions )","title":"Inference time"},{"location":"validations/#feature-importance","text":"Feature importance calculation for the following validations is based on sklearn's permutation_importance . Feature importance validation for top n features. Validates that a given feature is in the top n most important features. For calculation of feature importance we are using sklearn's permutation_importance. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_feature_in_top_n_important_features ( dataset = \"testing_data\" , feature = \"feature_a\" , top_n_features = 2 , ) Parameters: Name Type Description Default feature str feature to assess. required top_n_features int the number of most important features that the named feature must be ranked in. E.g. if top_n_features=2, the feature must be within the top two most important features. required dataset str the name of a dataset from the DataContext to calculate feature importance on {'testing_data', 'training_data'}. 'testing_data' permutation_kwargs Optional [ Dict [ str , Any ]] kwargs to pass into the sklearn.inspection.permutation_importance function. None Returns: Type Description True for success, false otherwise. With a results dictionary giving the actual feature importance ranking. Source code in trubrics/validations/model/base.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 @validation_output def validate_feature_in_top_n_important_features ( self , feature : str , top_n_features : int , dataset : str = \"testing_data\" , permutation_kwargs : Optional [ Dict [ str , Any ]] = None , severity : Optional [ str ] = None , ): \"\"\"**Feature importance validation for top n features.** Validates that a given feature is in the top n most important features. For calculation of feature \\ importance we are using sklearn's permutation_importance. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_feature_in_top_n_important_features( dataset=\"testing_data\", feature=\"feature_a\", top_n_features=2, ) ``` Args: feature: feature to assess. top_n_features: the number of most important features that the named feature must be ranked in. E.g. if top_n_features=2, the feature must be within the top two most important features. dataset: the name of a dataset from the DataContext to calculate feature importance on \\ {'testing_data', 'training_data'}. permutation_kwargs: kwargs to pass into the sklearn.inspection.permutation_importance function. Returns: True for success, false otherwise. With a results dictionary giving the actual feature importance ranking. \"\"\" return self . _validate_feature_in_top_n_important_features ( feature , top_n_features , dataset , permutation_kwargs ) Permutation feature importance validation between train and test sets. Validates that the ranking of top n features is the same for both test and train sets. For calculation of feature importance we are using sklearn's permutation_importance. Example from trubrics.validations import ModelValidator model_validator = ModelValidator ( data = data_context , model = model ) model_validator . validate_feature_importance_between_train_and_test ( top_n_features = 1 ) Parameters: Name Type Description Default top_n_features Optional [ int ] the number of most important features to consider for comparison between train and test sets. E.g. if top_n_features=2, the train and test sets must have the same 2 most important features, in the same order. None permutation_kwargs Optional [ Dict [ str , Any ]] kwargs to pass into the sklearn.inspection.permutation_importance function. None Returns: Type Description True for success, false otherwise. With a results dictionary giving the actual feature importance ranking. Source code in trubrics/validations/model/base.py 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 @validation_output def validate_feature_importance_between_train_and_test ( self , top_n_features : Optional [ int ] = None , permutation_kwargs : Optional [ Dict [ str , Any ]] = None ): \"\"\"**Permutation feature importance validation between train and test sets.** Validates that the ranking of top n features is the same for both test and train sets. For calculation of \\ feature importance we are using sklearn's permutation_importance. Example: ```py from trubrics.validations import ModelValidator model_validator = ModelValidator(data=data_context, model=model) model_validator.validate_feature_importance_between_train_and_test( top_n_features=1 ) ``` Args: top_n_features: the number of most important features to consider for comparison between train and test \\ sets. E.g. if top_n_features=2, the train and test sets must have the same 2 most \\ important features, in the same order. permutation_kwargs: kwargs to pass into the sklearn.inspection.permutation_importance function. Returns: True for success, false otherwise. With a results dictionary giving the actual feature importance ranking. \"\"\" return self . _validate_feature_importance_between_train_and_test ( top_n_features , permutation_kwargs )","title":"Feature Importance"}]}